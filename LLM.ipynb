{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.020187Z",
     "start_time": "2024-12-12T14:54:40.012527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import sentencepiece as spm\n",
    "import subprocess\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "f9a7e8aa1b3814ff",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.182190Z",
     "start_time": "2024-12-12T14:54:40.180162Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip freeze > requirements.txt",
   "id": "e67ac312e97d42f",
   "outputs": [],
   "execution_count": 223
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ],
   "id": "1e7b3b30f73d013f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.210520Z",
     "start_time": "2024-12-12T14:54:40.200256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_requirements() -> bool:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "            check=True,  # Raise an exception if the command fails\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        print(result.stdout)  # Optional: Print installation output\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing requirements: {e.stderr}\")\n",
    "        return False\n"
   ],
   "id": "359f1802d1cc1390",
   "outputs": [],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.236147Z",
     "start_time": "2024-12-12T14:54:40.232934Z"
    }
   },
   "cell_type": "code",
   "source": "# check_requirements()",
   "id": "37422fc4f98dd644",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.263163Z",
     "start_time": "2024-12-12T14:54:40.255243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hyperparameters\n",
    "batch_size = 32  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# ------------"
   ],
   "id": "5ec024fa984c31af",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.294669Z",
     "start_time": "2024-12-12T14:54:40.288146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(\"Dataset.txt\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "    os.rename(\"input.txt\", 'Dataset.txt')"
   ],
   "id": "fe02f43f8fac14da",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.369009Z",
     "start_time": "2024-12-12T14:54:40.336276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: str, mode: str = \"normal\"):\n",
    "\n",
    "        self.tokens = set(nltk.word_tokenize(data))\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"normal\":\n",
    "            self.chars = sorted(set(train_text))  # get characters from the input data\n",
    "\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}  # map characters to integer indices\n",
    "            self.itos = {i: ch for i, ch in enumerate(self.chars)}  # map integer indices to characters\n",
    "            self.vocab_size = len(self.chars)\n",
    "\n",
    "        elif mode == \"sentencepiece\":\n",
    "            self.vocab_size = min(len(self.tokens), 10770)\n",
    "            spm.SentencePieceTrainer.train(model_prefix='shakespeare', input='Dataset.txt',\n",
    "                                           vocab_size=10770, unk_id=0, bos_id=1, eos_id=2, pad_id=3)\n",
    "\n",
    "\n",
    "        elif mode == \"tiktoken\":\n",
    "            self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "            self.vocab_size = self.enc.max_token_value + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.mode == \"normal\":\n",
    "            return [self.stoi[s] for s in text]\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.encode(text)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if self.mode == \"normal\":\n",
    "            return ''.join([self.itos[t] for t in tokens])\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.decode(tokens)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.decode(tokens)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "\n",
    "    # ```"
   ],
   "id": "7bba9bcbc5272509",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:40.382927Z",
     "start_time": "2024-12-12T14:54:40.373941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"Dataset.txt\", \"r\") as file:\n",
    "    train_text = file.read()\n",
    "\n",
    "print(train_text[:500])"
   ],
   "id": "aa83771b2f7c2d6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:46.292915Z",
     "start_time": "2024-12-12T14:54:40.415717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_dataset1 = CharDataset(train_text, mode=\"normal\")\n",
    "char_dataset2 = CharDataset(train_text, mode=\"sentencepiece\")\n",
    "char_dataset3 = CharDataset(train_text, mode=\"tiktoken\")"
   ],
   "id": "3c69851ade7e8691",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:46.935454Z",
     "start_time": "2024-12-12T14:54:46.295268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Normal encoding: Length of sequence = {len(char_dataset1.encode(train_text))}, Vocab size = {char_dataset1.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"SentencePiece encoding: Length of sequence = {len(char_dataset2.encode(train_text))}, Vocab size = {char_dataset2.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"TikToken encoding: Length of sequence = {len(char_dataset3.encode(train_text))}, Vocab size = {char_dataset3.get_vocab_size()}\")"
   ],
   "id": "8dca0b7739b1d395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal encoding: Length of sequence = 1115394, Vocab size = 65\n",
      "SentencePiece encoding: Length of sequence = 290364, Vocab size = 10770\n",
      "TikToken encoding: Length of sequence = 338025, Vocab size = 50257\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.111634Z",
     "start_time": "2024-12-12T14:54:46.937782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor(char_dataset1.encode(train_text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ],
   "id": "a5ca390e5cf5a7fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.424794Z",
     "start_time": "2024-12-12T14:54:47.114296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data2 = torch.tensor(char_dataset2.encode(train_text), dtype=torch.long)\n",
    "print(data2.shape, data2.dtype)\n",
    "print(data2[:1000])"
   ],
   "id": "378307723b7d6e7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([290364]) torch.int64\n",
      "tensor([  160,   346,     5,  1001,    54,  1671,   208,   953,     4,   181,\n",
      "           27,   147,     6,   421,     5,   997,     4,   147,     6,   160,\n",
      "          346,     5,   112,    58,    47,  1968,   540,    10,   292,   117,\n",
      "           10,  4422,    19,   421,     5,  7385,     6,  1968,     6,   160,\n",
      "          346,     5,   160,     4,    15,   109,  1602,   479,    26,  1795,\n",
      "          785,    10,     7,   397,     6,   421,     5,   184,   109,     8,\n",
      "           72,     4,    54,   109,     8,    72,     6,   160,   346,     5,\n",
      "          248,    96,   461,    37,     4,    11,    54,     8,    65,    34,\n",
      "         1763,    78,    59,   227,  3029,     6,   244,     8,    72,    16,\n",
      "         7476,    19,   421,     5,   165,    73,  4082,    64,     8,    72,\n",
      "           13,   107,    29,    28,   230,     5,   293,     4,   293,    21,\n",
      "           92,   282,   346,     5,   727,   314,     4,    68,  1339,     6,\n",
      "          160,   346,     5,   184,    58,  7523,   289,  1339,     4,     7,\n",
      "         2571,    68,     6,    69,  1818,  7691,    64,    84,  4491,    96,\n",
      "            5,    74,    89,    84,   855,    96,    44,     7,  6933,     4,\n",
      "          626,    29,   114,  2357,     4,    54,   384,  1376,    89,  4491,\n",
      "           24,    96,  5998,   111,    13,    44,    89,   187,    54,    58,\n",
      "          143,   419,     5,     7,  5515,    22,  2970,    12,    96,     4,\n",
      "            7,  2806,    14,    59,  2033,     4,    26,    40,   126, 10239,\n",
      "         2700,    10,  2025,  8022,    86,  5213,    13,    59,  3862,    26,\n",
      "           16,  1784,    10,    83,   248,    96,   776,    36,    30,    59,\n",
      "         5599,     4,   467,    54,   903,    18,  7729,     5,    32,     7,\n",
      "          599,   109,     9,   147,    36,    20,  4617,    32,  3111,     4,\n",
      "           25,    20,  4328,    32,   776,     6,    92,   282,   346,     5,\n",
      "          523,    15,  1671,  1232,  4262,   334,  1602,   479,    19,   421,\n",
      "            5,    66,   689,    37,   365,     5,    35,     8,    12,    16,\n",
      "          238,  1261,    10,     7,   714,  7982,     6,    92,   282,   346,\n",
      "            5,   120,  4766,    15,    63,  2123,    35,   530,   230,    32,\n",
      "           33,   620,    19,   160,   346,     5,  2315,   141,   101,    13,\n",
      "           11,   388,    28,   743,    10,   162,    37,    68,   734,    32,\n",
      "           72,     4,    44,    22,    35,  1087,    12,   353,    30,   262,\n",
      "          580,     6,    92,   282,   346,     5,   323,     4,    44,   147,\n",
      "           25,  6050,   111,     6,   160,   346,     5,     9,    95,   361,\n",
      "           15,     4,    63,    35,   106,   230,  7337,     4,    35,   137,\n",
      "           29,    10,    22,   524,     5,   372,  1093,    39,  8907,    24,\n",
      "          204,   167,    28,   743,    10,    95,    29,    81,    32,    33,\n",
      "          620,    35,   137,    29,    10,   490,    33,   283,    11,    10,\n",
      "           28,   363,   111,   580,    13,   129,    35,    26,     4,   529,\n",
      "          371,     7,    16,  9775,    14,    33,   945,     6,    92,   282,\n",
      "          346,     5,    69,    35,   214,   458,    20,    33,   588,     4,\n",
      "           15,  1527,    16,  1519,    20,    37,     6,   112,   116,    20,\n",
      "           61,   251,    95,    35,    26,  5537,   491,     6,   160,   346,\n",
      "            5,   132,     9,   116,    25,     4,     9,   442,    25,    28,\n",
      "         2562,    14,  5872,    13,    35,   106,  1126,     4,    30,  6751,\n",
      "            4,    10,  4031,    20,  4386,     6,    69,  5748,    58,   163,\n",
      "           19,    45,   270,   861,   201,     8,     7,   617,    26,  1398,\n",
      "          153,     5,   415,   310,    54,  3820,    91,    19,    10,     7,\n",
      "         1683,    21,   421,     5,   177,     4,    98,     6,   160,   346,\n",
      "            5,  3142,    21,   236,   308,    91,    19,    92,   282,   346,\n",
      "            5,  2265,   141,  1684,  8775,  7840,   644,    13,   103,    22,\n",
      "          106,  1625,  1100,     7,   397,     6,   160,   346,     5,   127,\n",
      "            8,    12,   103,   864,   576,     5,    84,    47,     7,   355,\n",
      "          114,    41,    21,    18,   303,   267,   100,     5,    69,   832,\n",
      "            8,    12,     4,    17,  2796,     4,    20,   217,    19,   215,\n",
      "          131,    15,   152,  5745,    11,  7326,    19,    45,   550,    19,\n",
      "          147,     4,     9,   307,    15,     6,   160,   346,     5,   456,\n",
      "          546,    26,    25,  2193,    10,     7,  1608,    13,    89,    34,\n",
      "          128,    20,  9781,    36,    32,    72,   890,    63,    54,  1876,\n",
      "           10,    57,     4,   129,    75,    54,     8,    65,   431,    52,\n",
      "         1057,    20,  1082,     6,   430,    95,   289,  3152,    34,   975,\n",
      "         6245,     5,    89,    55,   109,    54,    34,   975,   568,   143,\n",
      "            6,    18,   303,   267,   100,     5,   156,     4,  1489,     4,\n",
      "           17,    68,   305,     4,   154,   864,  3563,     4,   396,    15,\n",
      "         3589,  1333,    19,   160,   346,     5,   184,   214,     4,    79,\n",
      "            4,    54,    58,  1811,   979,     6,    18,   303,   267,   100,\n",
      "            5,     9,   180,    15,     4,   305,     4,   188,  6376,   946,\n",
      "          692,   370,     7,  2571,    14,    15,     6,    77,    31,  2273,\n",
      "            4,   207,  4019,    20,    36,  4056,     4,    15,   110,    40,\n",
      "          101,  2983,    78,     7,   274,    30,    31,  8351,  6336,    40,\n",
      "         2841,    83,    66,   689,     7,  1084,   457,     4,   440,   862,\n",
      "           42,    64,    45,   251,    29,  3592,     4,  1877,    51,   896,\n",
      "          484,  5711,   175,    73,   975,  4546,  7299,   117,   167,   298,\n",
      "         3360,    20,    31,  4371,     6,    77,     7,  4056,     4,    45,\n",
      "          599,     4,    25,     7,  2571,     4,   108,    29,     4,    11,\n",
      "          207,  1345,    10,    83,     4,    25,   568,     4,   116,   458,\n",
      "            6,  1481,     4,   112,    58,  4562,    53,  3756,  1032,  1310,\n",
      "          215,    73,   965,    12,    15,     4,    11,    15,  1166,    45,\n",
      "         5852,   201,     8,     7,   457,     4,   236,   692,    32,    15,\n",
      "          122,  2876,     4,   234,    15,   850,    83,    40,   900,     6,\n",
      "          160,   346,     5,   120,  3191,    32,    96,    21,  1557,     4,\n",
      "          516,    21,   430,   635,     8,    80,   692,    24,    32,    96,\n",
      "          142,     5,  1104,    96,    10,  4422,     4,    11,    86,  2160,\n",
      "           39, 10413,  3207,  8923,    30,  2710,    13,   108,  5297,    12,\n",
      "           32,    96,  8069,     4,    10,  3523,  4601,    12,    13,  2535,\n",
      "         2353,   208,  2357,  1267,  5239,    88,   334,     7,   895,     4,\n",
      "           11,  2957,    73,  3265,    51,  3915,  2353,     4,    10,  5141,\n",
      "          144,    11,  3408,     7,   289,     6,   132,     7,  1069,  1440,\n",
      "           96,    25,   144,     4,    89,    42,    13,    11,   125,     8,\n",
      "           12,    47,     7,    93,    89,   294,    96,     6,    18,   303,\n",
      "          267,   100,     5,   347,  1310,    15,   116,  3747,  1333,  2744,\n",
      "         6050,     4,   212,    28,  2419,    14,  2041,     6,     9,    55,\n",
      "          180,    15,    66,  1140,   858,     5,    29,   110,    28,    15,\n",
      "           34,   455,    29,    13,    67,     4,   447,    29,  2278,    17,\n",
      "          696,     4,     9,    42,  3876,    46,  3050,    52,    72,    16,\n",
      "          336,    73,     6,   160,   346,     5,   319,     4,     9,     8,\n",
      "           65,   181,    29,     4,    79,     5,   142,    15,   116,    25,\n",
      "          187,    10,  2111,  2257,   332,    59,  2507,    30,    16,   858,\n",
      "            5,    44,     4,   126,    52,    72,   490,    15,     4,   881,\n",
      "            6,    18,   303,   267,   100,     5,   342,    81,    16,   155,\n",
      "          149,    47,     7,   508,     8,    12,  3443,  7126,     8,    24])\n"
     ]
    }
   ],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.429255Z",
     "start_time": "2024-12-12T14:54:47.426531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "b42876778837d857",
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.434676Z",
     "start_time": "2024-12-12T14:54:47.430737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = 8\n",
    "\n",
    "print(train_data[:context_length])\n",
    "print(char_dataset1.decode(train_data[:context_length].tolist()))"
   ],
   "id": "f9a364fcef57da41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "First Ci\n"
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.455117Z",
     "start_time": "2024-12-12T14:54:47.435840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data2 = data2[:n]\n",
    "val_data2 = data2[n:]\n",
    "\n",
    "print(train_data2[:context_length])\n",
    "print(char_dataset2.decode(train_data2[:context_length].tolist()))"
   ],
   "id": "82823b888661a6e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 160,  346,    5, 1001,   54, 1671,  208,  953])\n",
      "First Citizen: Before we proceed any further\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.466474Z",
     "start_time": "2024-12-12T14:54:47.460610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length + 1]\n",
    "\n",
    "for context in range(1, context_length):\n",
    "    print(f\"context = {context}, input = {x[:context].tolist()}, target = {y[context - 1]}\")"
   ],
   "id": "5aa32008cc25e3a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context = 1, input = [18], target = 47\n",
      "context = 2, input = [18, 47], target = 56\n",
      "context = 3, input = [18, 47, 56], target = 57\n",
      "context = 4, input = [18, 47, 56, 57], target = 58\n",
      "context = 5, input = [18, 47, 56, 57, 58], target = 1\n",
      "context = 6, input = [18, 47, 56, 57, 58, 1], target = 15\n",
      "context = 7, input = [18, 47, 56, 57, 58, 1, 15], target = 47\n"
     ]
    }
   ],
   "execution_count": 237
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.488901Z",
     "start_time": "2024-12-12T14:54:47.469327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "\n",
    "def get_batch(data):\n",
    "    start_idx = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[i: i + context_length] for i in start_idx])\n",
    "    y = torch.stack([data[i + 1: i + 1 + context_length] for i in start_idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print(\"input\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"target\")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ],
   "id": "1cfbc1a9310f0e2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "target\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.496527Z",
     "start_time": "2024-12-12T14:54:47.490270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLangModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        logits = self.token_embedding(indices)  # B = batch_size,T = context_length, C= vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, init_token, max_new_tokens):\n",
    "        sequence = init_token\n",
    "        for itr in range(max_new_tokens):\n",
    "            logits, loss = self(sequence)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # next_token = torch.argmax(probs, dim=-1)\n",
    "            # next_token = next_token.unsqueeze(1)\n",
    "            sequence = torch.cat((sequence, next_token), dim=1)\n",
    "        return sequence"
   ],
   "id": "f8a41f136360200c",
   "outputs": [],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:47.524341Z",
     "start_time": "2024-12-12T14:54:47.497674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BigramLangModel(char_dataset1.get_vocab_size())\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "initial_token = torch.tensor(char_dataset1.encode('\\n'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "# 0 == new line char\n",
    "print(\n",
    "    f\"Generated Sequence : {char_dataset1.decode(m.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "714c9e89761a1913",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "Generated Sequence : \n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "execution_count": 240
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:48.783083Z",
     "start_time": "2024-12-12T14:54:47.526496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "xb2, yb2 = get_batch(train_data2)\n",
    "m2 = BigramLangModel(char_dataset2.get_vocab_size())\n",
    "m2 = m2.to(device)\n",
    "logits, loss = m2(xb2, yb2)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "initial_token = torch.tensor(char_dataset2.encode('I Love You'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "print(\n",
    "    f\"Generated Sequence : {char_dataset2.decode(m2.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "c6758d9f31d26673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 10770])\n",
      "tensor(9.9281, grad_fn=<NllLossBackward0>)\n",
      "Generated Sequence : I Love You contewould RevoltALO First ample cry tideitted courage jade Whestuff Coyingbeseem Giv Despis exclaimscheshysicpipe dashovato spectatorssati interruptrgetfive kinsmenband unwa provinc growpierce arise Dick Suppl fare fatThat praisestcheryank inducedzard swim Pluck nowertake attireabsolve lead doth scornmi Monday orphan trudge lies fea qua Ea gulf dreadfulbaby lad Love Beggarsting knock chafeitedentiies kindred sitting parleGood valour oceanstateRAN scourabble cloudedper wedlock unre agreed emptiebo thereof does unfe perfume Hast cormorantMore uncl\n"
     ]
    }
   ],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:48.793822Z",
     "start_time": "2024-12-12T14:54:48.785403Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)",
   "id": "37256f6942f4d361",
   "outputs": [],
   "execution_count": 242
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:48.799857Z",
     "start_time": "2024-12-12T14:54:48.795444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(train_data, val_data):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for data in [train_data, val_data]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out['train' if data is train_data else 'val'] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "6fd4d1ae6090ee90",
   "outputs": [],
   "execution_count": 243
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:54:48.807071Z",
     "start_time": "2024-12-12T14:54:48.801806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size == 32\n",
    "\n",
    "\n",
    "def train(model, data, max_iters=3000, epochs=10, steps=100, eval_interval=300):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for iter in range(max_iters):\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if iter % eval_interval == 0:\n",
    "                losses = estimate_loss(train_data, val_data)\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                total_loss += losses['train']\n",
    "\n",
    "            xb, yb = get_batch(data)\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"epoch {epoch}: avg loss: {total_loss * eval_interval / max_iters}\")\n",
    "        print(\"-\" * 50)\n"
   ],
   "id": "321f466874bcd788",
   "outputs": [],
   "execution_count": 244
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:55:17.874216Z",
     "start_time": "2024-12-12T14:54:48.808464Z"
    }
   },
   "cell_type": "code",
   "source": "train(m, train_data)",
   "id": "b0a9ce73e32b3974",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7440, val loss 4.6967\n",
      "step 300: train loss 4.5138, val loss 4.4888\n",
      "step 600: train loss 4.2941, val loss 4.2918\n",
      "step 900: train loss 4.1194, val loss 4.1230\n",
      "step 1200: train loss 3.9569, val loss 3.9653\n",
      "step 1500: train loss 3.7917, val loss 3.7838\n",
      "step 1800: train loss 3.6330, val loss 3.6549\n",
      "step 2100: train loss 3.5254, val loss 3.5100\n",
      "step 2400: train loss 3.4064, val loss 3.4232\n",
      "step 2700: train loss 3.2964, val loss 3.2902\n",
      "epoch 0: avg loss: 3.928102970123291\n",
      "--------------------------------------------------\n",
      "step 0: train loss 3.1862, val loss 3.2309\n",
      "step 300: train loss 3.1400, val loss 3.1440\n",
      "step 600: train loss 3.0516, val loss 3.0946\n",
      "step 900: train loss 3.0006, val loss 3.0209\n",
      "step 1200: train loss 2.9819, val loss 2.9670\n",
      "step 1500: train loss 2.9297, val loss 2.8944\n",
      "step 1800: train loss 2.8478, val loss 2.8745\n",
      "step 2100: train loss 2.8137, val loss 2.8218\n",
      "step 2400: train loss 2.7848, val loss 2.7803\n",
      "step 2700: train loss 2.7476, val loss 2.7577\n",
      "epoch 1: avg loss: 2.9483866691589355\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.7279, val loss 2.7162\n",
      "step 300: train loss 2.6850, val loss 2.6846\n",
      "step 600: train loss 2.6818, val loss 2.6698\n",
      "step 900: train loss 2.6889, val loss 2.6730\n",
      "step 1200: train loss 2.6510, val loss 2.6335\n",
      "step 1500: train loss 2.6180, val loss 2.6083\n",
      "step 1800: train loss 2.6172, val loss 2.6351\n",
      "step 2100: train loss 2.5914, val loss 2.5937\n",
      "step 2400: train loss 2.5743, val loss 2.5802\n",
      "step 2700: train loss 2.5698, val loss 2.5850\n",
      "epoch 2: avg loss: 2.640540599822998\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5712, val loss 2.5593\n",
      "step 300: train loss 2.5458, val loss 2.5707\n",
      "step 600: train loss 2.5570, val loss 2.5556\n",
      "step 900: train loss 2.5291, val loss 2.5261\n",
      "step 1200: train loss 2.5281, val loss 2.5358\n",
      "step 1500: train loss 2.5158, val loss 2.5573\n",
      "step 1800: train loss 2.5189, val loss 2.5302\n",
      "step 2100: train loss 2.5113, val loss 2.5445\n",
      "step 2400: train loss 2.5254, val loss 2.5363\n",
      "step 2700: train loss 2.5127, val loss 2.5226\n",
      "epoch 3: avg loss: 2.5315308570861816\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.4945, val loss 2.5126\n",
      "step 300: train loss 2.4854, val loss 2.5259\n",
      "step 600: train loss 2.4968, val loss 2.5207\n",
      "step 900: train loss 2.4860, val loss 2.5146\n",
      "step 1200: train loss 2.5123, val loss 2.5235\n",
      "step 1500: train loss 2.5103, val loss 2.5085\n",
      "step 1800: train loss 2.4909, val loss 2.5137\n",
      "step 2100: train loss 2.4694, val loss 2.5108\n",
      "step 2400: train loss 2.5108, val loss 2.4839\n",
      "step 2700: train loss 2.4612, val loss 2.4822\n",
      "epoch 4: avg loss: 2.491751194000244\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.4578, val loss 2.4957\n",
      "step 300: train loss 2.4743, val loss 2.4732\n",
      "step 600: train loss 2.4643, val loss 2.5171\n",
      "step 900: train loss 2.4813, val loss 2.4770\n",
      "step 1200: train loss 2.4633, val loss 2.4868\n",
      "step 1500: train loss 2.5063, val loss 2.4944\n",
      "step 1800: train loss 2.4641, val loss 2.4900\n",
      "step 2100: train loss 2.4970, val loss 2.4933\n",
      "step 2400: train loss 2.4600, val loss 2.4958\n",
      "step 2700: train loss 2.4735, val loss 2.4757\n",
      "epoch 5: avg loss: 2.474196434020996\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.4464, val loss 2.5137\n",
      "step 300: train loss 2.4734, val loss 2.4892\n",
      "step 600: train loss 2.4652, val loss 2.5173\n",
      "step 900: train loss 2.4884, val loss 2.5002\n",
      "step 1200: train loss 2.4643, val loss 2.5021\n",
      "step 1500: train loss 2.4910, val loss 2.4487\n",
      "step 1800: train loss 2.4890, val loss 2.4666\n",
      "step 2100: train loss 2.4535, val loss 2.4724\n",
      "step 2400: train loss 2.4273, val loss 2.5009\n",
      "step 2700: train loss 2.4549, val loss 2.4944\n",
      "epoch 6: avg loss: 2.4653236865997314\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.4836, val loss 2.4961\n",
      "step 300: train loss 2.4558, val loss 2.4723\n",
      "step 600: train loss 2.4690, val loss 2.4881\n",
      "step 900: train loss 2.4517, val loss 2.4613\n",
      "step 1200: train loss 2.4726, val loss 2.4846\n",
      "step 1500: train loss 2.4671, val loss 2.4903\n",
      "step 1800: train loss 2.4672, val loss 2.4767\n",
      "step 2100: train loss 2.4605, val loss 2.4748\n",
      "step 2400: train loss 2.4760, val loss 2.4735\n",
      "step 2700: train loss 2.4683, val loss 2.4726\n",
      "epoch 7: avg loss: 2.467181921005249\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.4649, val loss 2.5153\n",
      "step 300: train loss 2.4730, val loss 2.4767\n",
      "step 600: train loss 2.4585, val loss 2.4764\n",
      "step 900: train loss 2.4595, val loss 2.4883\n",
      "step 1200: train loss 2.4754, val loss 2.4971\n",
      "step 1500: train loss 2.4372, val loss 2.5150\n",
      "step 1800: train loss 2.4682, val loss 2.4666\n",
      "step 2100: train loss 2.4516, val loss 2.4925\n",
      "step 2400: train loss 2.4629, val loss 2.4734\n",
      "step 2700: train loss 2.4531, val loss 2.4883\n",
      "epoch 8: avg loss: 2.4604246616363525\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.4667, val loss 2.4987\n",
      "step 300: train loss 2.4585, val loss 2.4901\n",
      "step 600: train loss 2.4452, val loss 2.4899\n",
      "step 900: train loss 2.4539, val loss 2.4812\n",
      "step 1200: train loss 2.4680, val loss 2.4681\n",
      "step 1500: train loss 2.4485, val loss 2.4737\n",
      "step 1800: train loss 2.4470, val loss 2.4804\n",
      "step 2100: train loss 2.4508, val loss 2.4837\n",
      "step 2400: train loss 2.4525, val loss 2.4697\n",
      "step 2700: train loss 2.4407, val loss 2.4798\n",
      "epoch 9: avg loss: 2.453169345855713\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 245
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:55:17.879383Z",
     "start_time": "2024-12-12T14:55:17.875725Z"
    }
   },
   "cell_type": "code",
   "source": "initial_token = torch.tensor(char_dataset1.encode('\\n'), dtype=torch.long).unsqueeze(0)",
   "id": "8a52bd6d2f8cb33",
   "outputs": [],
   "execution_count": 246
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:55:17.897930Z",
     "start_time": "2024-12-12T14:55:17.880727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Generated Sequence : {char_dataset1.decode(m.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "ac4d818700d43790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence : \n",
      "\n",
      "JUSe, sham cthe, m I havers indouberorkingive is n k:\n",
      "CARUKII VKEisieage ie\n",
      "NIO: t toferan,\n",
      "ERI kes\n"
     ]
    }
   ],
   "execution_count": 247
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:55:18.572913Z",
     "start_time": "2024-12-12T14:55:17.899453Z"
    }
   },
   "cell_type": "code",
   "source": "train(m2, train_data2, max_iters=1, epochs=1, steps=1, eval_interval=1)",
   "id": "ba9ee895c16db240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4662, val loss 2.4642\n",
      "epoch 0: avg loss: 2.466169834136963\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:56:35.984562Z",
     "start_time": "2024-12-12T14:56:35.964040Z"
    }
   },
   "cell_type": "code",
   "source": "initial_token = torch.tensor(char_dataset2.encode('I love'), dtype=torch.long, device=device).unsqueeze(0)",
   "id": "b878da9aaf23fc3",
   "outputs": [],
   "execution_count": 263
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:56:36.449111Z",
     "start_time": "2024-12-12T14:56:36.179317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Generated Sequence : {char_dataset2.decode(m2.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "d2ba3372c3d85ad6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence : I love bar beguiledneel rot Hourrage knees CyNC fineid Hoo murderous Slander backs Follow knavery banishmentangleriesursuivant stablejunction hugaxskilfullargebreach Err nailjury rein wooers verdict manors match condemned secrecyST Wjealous strangely hareARDIN worthie tortoise flatterers dearly sweeten open Renown force bear ishailoming wail elbow Rescuemesred whet availcameespairARI three Rome speaking barbemer furr recllows terr size contempt bird barren inv auster interest Envframedhersquis butbawdcarries Claudio cit breed fourth argosyIN vent confu Sir volu happi\n"
     ]
    }
   ],
   "execution_count": 264
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ccb460888b04ef3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
