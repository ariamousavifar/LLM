{
 "cells": [
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9a7e8aa1b3814ff",
    "outputId": "f79ba14f-d83b-4a7a-be6f-2af7780f9c8e",
    "ExecuteTime": {
     "end_time": "2024-12-16T12:53:54.573735Z",
     "start_time": "2024-12-16T12:53:09.719881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import sentencepiece as spm\n",
    "import subprocess\n",
    "import wandb\n",
    "import nltk\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "f9a7e8aa1b3814ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/aria/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:47.871439Z",
     "start_time": "2024-12-13T18:43:47.861997Z"
    },
    "id": "e67ac312e97d42f"
   },
   "cell_type": "code",
   "source": [
    "# !pip freeze > requirements.txt"
   ],
   "id": "e67ac312e97d42f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "93842d3d2aa663ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:47.943789Z",
     "start_time": "2024-12-13T18:43:47.940165Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bca6edd824b1b033",
    "outputId": "421ebbfa-168e-4cb4-834e-74a44008f46b"
   },
   "cell_type": "code",
   "source": [
    "# Login to Weights & Biases for experiment tracking\n",
    "wandb.login(key=\"2b242cad61896bc77d8053286a9c3e79f01c9127\")"
   ],
   "id": "bca6edd824b1b033",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.012871Z",
     "start_time": "2024-12-13T18:43:47.997947Z"
    },
    "id": "359f1802d1cc1390"
   },
   "cell_type": "code",
   "source": [
    "def check_requirements() -> bool:\n",
    "    \"\"\"\n",
    "    Check and install project requirements from requirements.txt.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if requirements are successfully installed, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(\"requirements.txt\"):\n",
    "            raise FileNotFoundError(\"requirements.txt not found\")\n",
    "\n",
    "        # Attempt to install requirements\n",
    "        result = subprocess.run(\n",
    "            [\"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "            check=True,  # Raise an exception if the command fails\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing requirements: {e.stderr}\")\n",
    "        return False"
   ],
   "id": "359f1802d1cc1390",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.099637Z",
     "start_time": "2024-12-13T18:43:48.095925Z"
    },
    "id": "37422fc4f98dd644"
   },
   "cell_type": "code",
   "source": [
    "# check_requirements()"
   ],
   "id": "37422fc4f98dd644",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.200417Z",
     "start_time": "2024-12-13T18:43:48.196944Z"
    },
    "id": "fe02f43f8fac14da"
   },
   "cell_type": "code",
   "source": [
    "# Download dataset if not exists\n",
    "if not os.path.exists(\"Dataset.txt\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "    os.rename(\"input.txt\", 'Dataset.txt')"
   ],
   "id": "fe02f43f8fac14da",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.231540Z",
     "start_time": "2024-12-13T18:43:48.205441Z"
    },
    "id": "7bba9bcbc5272509"
   },
   "cell_type": "code",
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for character-level and token-level encoding.\n",
    "    Supports three encoding modes: normal, sentencepiece, and tiktoken.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: str, mode: str = \"normal\"):\n",
    "        # Extract unique tokens and initialize encoding mode\n",
    "        self.tokens = set(nltk.word_tokenize(data))\n",
    "        self.mode = mode\n",
    "\n",
    "        # Normal encoding: Character-based tokenization\n",
    "        if mode == \"normal\":\n",
    "\n",
    "            self.chars = sorted(set(train_text))\n",
    "\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "            self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "            self.vocab_size = len(self.chars)\n",
    "\n",
    "        # SentencePiece encoding: Subword-based tokenization\n",
    "        elif mode == \"sentencepiece\":\n",
    "            self.vocab_size = min(len(self.tokens), 10770)\n",
    "            spm.SentencePieceTrainer.train(model_prefix='shakespeare', input='Dataset.txt',\n",
    "                                           vocab_size=10770, unk_id=0, bos_id=1, eos_id=2, pad_id=3)\n",
    "\n",
    "        # Tiktoken encoding: GPT-2 tokenization\n",
    "        elif mode == \"tiktoken\":\n",
    "            self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "            self.vocab_size = self.enc.max_token_value + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text into token IDs based on the selected mode.\n",
    "        \"\"\"\n",
    "        if self.mode == \"normal\":\n",
    "            return [self.stoi[s] for s in text]\n",
    "\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.encode(text)\n",
    "\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "        Decode token IDs back into text based on the selected mode.\n",
    "        \"\"\"\n",
    "        if self.mode == \"normal\":\n",
    "            return ''.join([self.itos[t] for t in tokens])\n",
    "\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.decode(tokens)\n",
    "\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.decode(tokens)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        Return the vocabulary size of the dataset.\n",
    "        \"\"\"\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset.\n",
    "        \"\"\"\n",
    "        return self.vocab_size"
   ],
   "id": "7bba9bcbc5272509",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.254518Z",
     "start_time": "2024-12-13T18:43:48.234893Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa83771b2f7c2d6e",
    "outputId": "54cb762c-41e2-48fe-a8e2-741c3870647e"
   },
   "cell_type": "code",
   "source": [
    "# Read dataset text\n",
    "with open(\"Dataset.txt\", \"r\") as file:\n",
    "    train_text = file.read()\n",
    "\n",
    "print(train_text[:500])"
   ],
   "id": "aa83771b2f7c2d6e",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:53.885925Z",
     "start_time": "2024-12-13T18:43:48.276236Z"
    },
    "id": "3c69851ade7e8691"
   },
   "cell_type": "code",
   "source": [
    "# Create datasets for different encoding modes\n",
    "normal_encoding = CharDataset(train_text, mode=\"normal\")\n",
    "sent_piece = CharDataset(train_text, mode=\"sentencepiece\")\n",
    "tiktoken_encoding = CharDataset(train_text, mode=\"tiktoken\")"
   ],
   "id": "3c69851ade7e8691",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:54.558538Z",
     "start_time": "2024-12-13T18:43:53.887996Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dca0b7739b1d395",
    "outputId": "bfaf8ee3-56e3-4700-ff67-d4c67995e59f"
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Normal encoding: Length of sequence = {len(normal_encoding.encode(train_text))}, Vocab size = {normal_encoding.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"SentencePiece encoding: Length of sequence = {len(sent_piece.encode(train_text))}, Vocab size = {sent_piece.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"TikToken encoding: Length of sequence = {len(tiktoken_encoding.encode(train_text))}, Vocab size = {tiktoken_encoding.get_vocab_size()}\")"
   ],
   "id": "8dca0b7739b1d395",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal encoding: Length of sequence = 1115394, Vocab size = 65\n",
      "SentencePiece encoding: Length of sequence = 290364, Vocab size = 10770\n",
      "TikToken encoding: Length of sequence = 338025, Vocab size = 50257\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "def get_batch(data, context_length, batch_size, device):\n",
    "    start_idx = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[i: i + context_length] for i in start_idx])\n",
    "    y = torch.stack([data[i + 1: i + 1 + context_length] for i in start_idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "metadata": {
    "id": "Frbr5VJomsgj"
   },
   "id": "Frbr5VJomsgj",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:24.001576Z",
     "start_time": "2024-12-13T19:40:23.938891Z"
    },
    "id": "ad93f3d25ee36141"
   },
   "cell_type": "code",
   "source": [
    "# Self-attention head definition\n",
    "class Head(nn.Module):\n",
    "    \"\"\" \n",
    "    One head of self-attention \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, context_length, n_embd, temperature, dropout, bias):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))  # Lower triangular matrix\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through a single self-attention head.\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "\n",
    "        # Attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "\n",
    "        # Masking upper triangle\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "\n",
    "        # Softmax attention weights\n",
    "        wei = F.softmax(wei / self.temperature, dim=-1)  # (B, T, T)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Weighted sum of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "        return out"
   ],
   "id": "ad93f3d25ee36141",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:24.337064Z",
     "start_time": "2024-12-13T19:40:24.330441Z"
    },
    "id": "927f800673ef1d3e"
   },
   "cell_type": "code",
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \"\"\" \n",
    "    A multi-head attention layer \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_head, head_size, context_length, n_embd, temperature, dropout, bias):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size, context_length=context_length, n_embd=n_embd,\n",
    "                                         temperature=temperature, dropout=dropout, bias=bias) for _ in range(num_head)])\n",
    "\n",
    "        # Final projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the multi-head attention layer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate outputs of all heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        return self.dropout(self.proj(out))"
   ],
   "id": "927f800673ef1d3e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:24.891698Z",
     "start_time": "2024-12-13T19:40:24.886757Z"
    },
    "id": "e40ea3153f88a992"
   },
   "cell_type": "code",
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" \n",
    "    A simple linear layer followed by a non-linearity \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "e40ea3153f88a992",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:25.549310Z",
     "start_time": "2024-12-13T19:40:25.544271Z"
    },
    "id": "e73bcc4f6c617ba8"
   },
   "cell_type": "code",
   "source": [
    "# Transformer block: Self-attention + Feed-forward network\n",
    "class Block(nn.Module):\n",
    "    \"\"\" \n",
    "    Transformer block: communication followed by computation \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_head, context_length, n_embd, temperature, dropout, bias):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_head\n",
    "        self.sa = MultiHead(num_head, head_size, context_length, n_embd, temperature, dropout, bias)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "\n",
    "        # Add residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x\n"
   ],
   "id": "e73bcc4f6c617ba8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:26.604470Z",
     "start_time": "2024-12-13T19:40:26.561595Z"
    },
    "id": "b7ccc59875cc7aee"
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm1d:  # (used to be BatchNorm1d)\n",
    "    \"\"\"\n",
    "    Implements Layer Normalization for 1D inputs.\n",
    "\n",
    "    Unlike Batch Normalization, Layer Normalization normalizes across features for each input independently.\n",
    "    This can be more effective for sequence data or when batch size is small.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the LayerNorm1d instance.\n",
    "        \"\"\"\n",
    "        self.eps = eps  # Learnable parameters for scaling and shifting the normalized data\n",
    "        self.gamma = torch.ones(dim)  # Scaling parameter\n",
    "        self.beta = torch.zeros(dim)  # Scaling parameter\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for LayerNorm1d.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate mean and variance along the feature dimension\n",
    "        xmean = x.mean(1, keepdim=True)  # batch mean\n",
    "        xvar = x.var(1, keepdim=True)  # batch variance\n",
    "\n",
    "        # Normalize the input to have zero mean and unit variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "\n",
    "        # Scale and shift using learnable parameters\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the list of learnable parameters.\n",
    "\n",
    "        Returns:\n",
    "            list: [gamma, beta]\n",
    "        \"\"\"\n",
    "\n",
    "        return [self.gamma, self.beta]"
   ],
   "id": "b7ccc59875cc7aee",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:49:07.879915Z",
     "start_time": "2024-12-13T19:49:07.767627Z"
    },
    "id": "f8a41f136360200c"
   },
   "cell_type": "code",
   "source": [
    "# torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLangModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Bigram Language Model with support for self-attention, feedforward layers, and token generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_layer, num_head=8, head_size=16, context_length=8, n_embed=32, temperature=1.0,\n",
    "                 dropout=0.0,\n",
    "                 bias=False):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position = nn.Embedding(context_length, n_embed)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(num_head=num_head, context_length=context_length, n_embd=n_embed,\n",
    "                                            temperature=temperature, dropout=dropout, bias=bias) for _ in\n",
    "                                      range(num_layer)])\n",
    "\n",
    "        # Layer normalization and feedforward layers\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.feedforward = FeedFoward(n_embed, dropout)\n",
    "\n",
    "        # Output head for generating logits\n",
    "        self.langhead = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # T: sequence length (number of tokens) , B: batch size (number of sequences)\n",
    "        B, T = indices.shape\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        tok_embeds = self.token_embedding(indices)  # (B, T, n_embed)\n",
    "        pos_embeds = self.position(torch.arange(T, device=indices.device))  # (T, n_embed)\n",
    "        x = tok_embeds + pos_embeds  # (B, T, n_embed)\n",
    "\n",
    "        # Transformer blocks and feedforward\n",
    "        x = self.blocks(x)\n",
    "        x = self.feedforward(x)  # (B, T, n_embed)\n",
    "\n",
    "        # Output logits\n",
    "        logits = self.langhead(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, init_token, max_new_tokens, context_length):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \"\"\"\n",
    "\n",
    "        sequence = init_token  # Initial sequence\n",
    "        for itr in range(max_new_tokens):\n",
    "            # Crop context to fit within context_length\n",
    "            sequence_cropped = sequence[:, -context_length:]\n",
    "\n",
    "            # Forward pass to compute logits\n",
    "            logits, loss = self(sequence_cropped)\n",
    "\n",
    "            # Focus on the last token in the sequence\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append the generated token to the sequence\n",
    "            sequence = torch.cat((sequence, next_token), dim=1)\n",
    "\n",
    "        return sequence"
   ],
   "id": "f8a41f136360200c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:50:22.393172Z",
     "start_time": "2024-12-13T19:50:22.363418Z"
    },
    "id": "6fd4d1ae6090ee90"
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, eval_iters, context_length, batch_size, device):\n",
    "    \"\"\"\n",
    "    Estimate the training and validation loss of a model over a specified number of evaluation iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    out = {}\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for data in [train_data, val_data]:\n",
    "        # Initialize a tensor to store losses for each evaluation iteration\n",
    "        losses = torch.zeros(eval_iters)\n",
    "\n",
    "        # Loop through the specified number of evaluation iterations\n",
    "        for k in range(eval_iters):\n",
    "            # Sample a batch of input-target pairs\n",
    "            X, Y = get_batch(data, context_length, batch_size, device)\n",
    "\n",
    "            # Forward pass to calculate loss\n",
    "            logits, loss = model(X, Y)\n",
    "\n",
    "            # Store the loss value\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        # Compute the average loss for the dataset (train or validation)\n",
    "        out['train' if data is train_data else 'val'] = losses.mean()\n",
    "\n",
    "    model.train()  # Reset the model to training mode\n",
    "\n",
    "    return out"
   ],
   "id": "6fd4d1ae6090ee90",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:50:24.013007Z",
     "start_time": "2024-12-13T19:50:23.999081Z"
    },
    "id": "321f466874bcd788"
   },
   "cell_type": "code",
   "source": [
    "def train(model, data, val_data, context_length, batch_size, device, max_iters=5000, epochs=10, steps=100,\n",
    "          eval_iters=200, eval_interval=100, learning_rate=1e-3, wandb_log=True):\n",
    "    \"\"\"\n",
    "    Trains a language model on the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Adam optimizer with the model parameters and specified learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for iter in range(max_iters):\n",
    "            # Evaluate model performance on train and validation sets at specified intervals\n",
    "            if iter % eval_interval == 0:\n",
    "                losses = estimate_loss(model, data, val_data, eval_iters, context_length, batch_size, device)\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                total_loss += losses['train']  # Add training loss for averaging\n",
    "\n",
    "                if wandb_log:\n",
    "                    # Log training and validation losses to Weights & Biases\n",
    "                    wandb.log({\"Iteration\": iter, \"Train Loss\": losses['train'], \"Val Loss\": losses['val']})\n",
    "\n",
    "            # Get a batch of training data\n",
    "            xb, yb = get_batch(data, context_length, batch_size, device)\n",
    "\n",
    "            # Perform a forward pass and compute loss\n",
    "            logits, loss = model(xb, yb)\n",
    "\n",
    "            # Zero out gradients from the previous step, backpropagate, and update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"epoch {epoch}: avg loss: {total_loss * eval_interval / max_iters}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if wandb_log:\n",
    "            # Log the total loss for the epoch to Weights & Biases\n",
    "            wandb.log({\"Epoch\": epoch + 1, \"Total Loss\": total_loss * eval_interval / max_iters})\n"
   ],
   "id": "321f466874bcd788",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_text(model, encoding, initial_text: str, max_new_tokens: int, device, context_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generates text from a given initial input using a trained language model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Encode the initial input text into token IDs using the encoding object\n",
    "    initial_token = torch.tensor(encoding.encode(initial_text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    # `unsqueeze(0)` adds a batch dimension, turning the tensor into (1, T) where T is the token sequence length.\n",
    "\n",
    "    # Step 2: Generate new tokens using the model\n",
    "    generated_text = encoding.decode(\n",
    "        model.generate(context_length=context_length, init_token=initial_token, max_new_tokens=max_new_tokens)[\n",
    "            0].tolist())  # We only need the first sequence in the batch, hence the [0] and convert it to a list.\n",
    "\n",
    "    return generated_text"
   ],
   "metadata": {
    "id": "NKVwj9W-HLHy"
   },
   "id": "NKVwj9W-HLHy",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def save_model(model, encoding, parameters):\n",
    "    \"\"\"\n",
    "   Saves the trained model, encoding object, and additional parameters to a file.\n",
    "   \"\"\"\n",
    "    i = 0\n",
    "    path = f\"./model{i}.pth\"\n",
    "    # Check if the path already exists. If it does, increment 'i' to create a unique file name.\n",
    "    while os.path.exists(path):\n",
    "        i += 1\n",
    "        path = f\"./model{i}.pth\"\n",
    "\n",
    "    # Step 2: Save the model state, encoding object, and parameters to the determined file path\n",
    "    torch.save(\n",
    "        dict(\n",
    "            model=model.state_dict(),\n",
    "            encoding=encoding,\n",
    "            parameters=parameters\n",
    "        ), path)"
   ],
   "metadata": {
    "id": "wWDD82QKLLwo"
   },
   "id": "wWDD82QKLLwo",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_save(dataset_name: str, encoding: str, parameters: dict, wandb_log=False):\n",
    "    \"\"\"\n",
    "     Trains a Bigram Language Model on a given dataset and saves the model. Optionally logs training progress to Weights & Biases.\n",
    "     \"\"\"\n",
    "\n",
    "    # Load the training data from the dataset file\n",
    "    with open(dataset_name, \"r\") as file:\n",
    "        train_text = file.read()\n",
    "\n",
    "    # Initialize encoding (used for tokenizing the text)\n",
    "    encoding_name = encoding\n",
    "    encoding = CharDataset(train_text, mode=encoding)\n",
    "\n",
    "    # Encode the text into numeric format\n",
    "    data = torch.tensor(normal_encoding.encode(train_text), dtype=torch.long)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    n = int(parameters[\"train_rate\"] * len(data))  # Use a fraction of the data for training\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    # Prepare a batch of data for training\n",
    "    xb, yb = get_batch(train_data, context_length=parameters['context_length'], batch_size=parameters['batch_size'],\n",
    "                       device=parameters[\"device\"])\n",
    "\n",
    "    # Initialize the model\n",
    "    model = BigramLangModel(\n",
    "        parameters[\"vocab_size\"],\n",
    "        parameters[\"num_layer\"],\n",
    "        n_embed=parameters[\"n_embed\"],\n",
    "        context_length=parameters[\"context_length\"],\n",
    "        temperature=parameters[\"temperature\"],\n",
    "        dropout=parameters[\"dropout\"],\n",
    "        num_head=parameters[\"num_head\"],\n",
    "        head_size=parameters[\"head_size\"])\n",
    "\n",
    "    # Move the model to the specified device (CPU or GPU)\n",
    "    m = model.to(parameters[\"device\"])\n",
    "\n",
    "    # Perform a forward pass and calculate the loss (for debugging/checking model initialization)\n",
    "    logits, loss = m(indices=xb, targets=yb)\n",
    "\n",
    "    # Initialize Weights & Biases logging if enabled\n",
    "    if wandb_log:\n",
    "        wandb.init(\n",
    "            project=\"LLM\",\n",
    "            config={\n",
    "                \"learning_rate\": parameters[\"learning_rate\"],\n",
    "                \"architecture\": \"Transformers\",\n",
    "                \"dataset\": \"Shakespeare\",\n",
    "            },\n",
    "\n",
    "            name=encoding_name\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    train(\n",
    "        model=m,\n",
    "        data=train_data,\n",
    "        val_data=val_data,\n",
    "        context_length=parameters['context_length'],\n",
    "        batch_size=parameters['batch_size'],\n",
    "        device=parameters[\"device\"],\n",
    "        learning_rate=parameters[\"learning_rate\"],\n",
    "        max_iters=parameters[\"max_iters\"],\n",
    "        epochs=parameters[\"epochs\"],\n",
    "        steps=parameters[\"steps\"],\n",
    "        eval_interval=parameters[\"eval_interval\"],\n",
    "        wandb_log=wandb_log)\n",
    "\n",
    "    # Generate some text after training\n",
    "    generated_text = generate_text(m, encoding, \"I love\", 100, parameters[\"device\"], parameters['context_length'])\n",
    "\n",
    "    # og generated text to Weights & Biases if enabled\n",
    "    if wandb_log:\n",
    "        wandb.log({\"Generated Text\": generated_text})\n",
    "        wandb.finish()\n",
    "\n",
    "    # Save the model, encoding, and parameters\n",
    "    save_model(m, encoding, parameters)\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "id": "lQ-lsPUF1ehA"
   },
   "id": "lQ-lsPUF1ehA",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model from the specified checkpoint file and returns the model along with the hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        # Load the file, mapping the model to the CPU\n",
    "        loaded_file = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "        # Extract the hyperparameters from the file\n",
    "        hyperparameters = loaded_file[\"parameters\"]\n",
    "\n",
    "        # Calculate head_size based on the number of heads and embedding size\n",
    "        num_head = hyperparameters[\"num_head\"]\n",
    "        head_size = hyperparameters[\"n_embed\"] // num_head\n",
    "\n",
    "        model = BigramLangModel(\n",
    "            vocab_size=hyperparameters[\"vocab_size\"],\n",
    "            num_layer=hyperparameters[\"num_layer\"],\n",
    "            n_embed=hyperparameters[\"n_embed\"],\n",
    "            context_length=hyperparameters[\"context_length\"],\n",
    "            temperature=hyperparameters[\"temperature\"],\n",
    "            dropout=hyperparameters[\"dropout\"],\n",
    "            num_head=hyperparameters[\"num_head\"],\n",
    "            head_size=hyperparameters[\"head_size\"]\n",
    "        )\n",
    "\n",
    "        # Load the saved model state dict into the model\n",
    "        model.load_state_dict(loaded_file[\"model\"])\n",
    "\n",
    "        return model, hyperparameters\n",
    "\n",
    "    else:\n",
    "        # If the checkpoint file does not exist,\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "id": "OhgvQ2MyV4jj"
   },
   "id": "OhgvQ2MyV4jj",
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_and_generate(model_path: str, encoding, initial_text: str, max_new_tokens: int):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model from the specified checkpoint file and generate text using the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model and hyperparameters\n",
    "    if not load_model(model_path):\n",
    "        print(\"Model not found\")\n",
    "        return\n",
    "\n",
    "    # Load the model and hyperparameters\n",
    "    model, parameters = load_model(model_path)\n",
    "\n",
    "    # Ensure the model is using the correct context length\n",
    "    context_length = parameters[\"context_length\"]\n",
    "    # vocab_size = encoding.get_vocab_size()\n",
    "\n",
    "    # # Correct the vocab_size in the parameters dictionary:\n",
    "    # parameters[\"vocab_size\"] = vocab_size\n",
    "\n",
    "    # # Ensure model is using the correct vocabulary size:\n",
    "    # model.token_embedding = nn.Embedding(vocab_size, model.n_embed)\n",
    "    # model.langhead = nn.Linear(model.n_embed, vocab_size)\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate text using the loaded model\n",
    "    generated_text = generate_text(model, encoding, initial_text, max_new_tokens, device, context_length)\n",
    "\n",
    "    return generated_text"
   ],
   "metadata": {
    "id": "qmoRznHwZTEm"
   },
   "id": "qmoRznHwZTEm",
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "hyperparameters = {\n",
    "    \"batch_size\": 64,  # Batch size for training\n",
    "    \"context_length\": 256,  # Number of tokens in the context for language modeling\n",
    "    \"max_iters\": 5000,  # Maximum number of iterations to train the model\n",
    "    \"eval_interval\": 500,  # Interval between evaluations during training\n",
    "    \"learning_rate\": 3e-4,  # Learning rate for the optimizer\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',  # Whether to use GPU or CPU\n",
    "    \"eval_iters\": 200,  # Number of iterations for evaluation per eval interval\n",
    "    \"n_embed\": 384,  # Size of the token embedding vector\n",
    "    \"num_head\": 4,  # Number of attention heads in the multi-head attention mechanism\n",
    "    \"num_layer\": 6,  # Number of transformer layers (blocks)\n",
    "    \"dropout\": 0.2,  # Dropout rate for regularization\n",
    "    \"temperature\": 1.0,  # Temperature scaling for softmax during generation (controls randomness)\n",
    "    \"epochs\": 1,  # Number of training epochs\n",
    "    \"train_rate\": 0.9,  # Fraction of data to be used for training (remaining goes for validation)\n",
    "    \"vocab_size\": normal_encoding.get_vocab_size(),  # Size of the vocabulary, retrieved from encoding\n",
    "    \"steps\": 500,  # Number of steps for each training iteration\n",
    "    \"bias\": False  # Whether to use bias in the linear layers\n",
    "}\n",
    "\n",
    "# Calculate the head size based on the number of heads and embedding size\n",
    "hyperparameters[\"head_size\"] = hyperparameters[\"n_embed\"] // hyperparameters[\"num_head\"]"
   ],
   "metadata": {
    "id": "OqtHi0-hdiDL"
   },
   "id": "OqtHi0-hdiDL",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "m = train_save(dataset_name=\"Dataset.txt\", encoding=\"normal\", parameters=hyperparameters, wandb_log=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "naWEIW6N1itW",
    "outputId": "14cffd8f-911f-44ab-9b7b-0ebef61eca83"
   },
   "id": "naWEIW6N1itW",
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mariamosavefar\u001B[0m (\u001B[33mariamosavefar-universit-de-gen-ve\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241216_001252-attjdcbk</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk' target=\"_blank\">normal</a></strong> to <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0: train loss 4.2228, val loss 4.2235\n",
      "step 500: train loss 1.8823, val loss 1.9928\n",
      "step 1000: train loss 1.5255, val loss 1.7086\n",
      "step 1500: train loss 1.3978, val loss 1.6077\n",
      "step 2000: train loss 1.3216, val loss 1.5537\n",
      "step 2500: train loss 1.2686, val loss 1.5147\n",
      "step 3000: train loss 1.2196, val loss 1.5030\n",
      "step 3500: train loss 1.1805, val loss 1.4883\n",
      "step 4000: train loss 1.1417, val loss 1.4980\n",
      "step 4500: train loss 1.1076, val loss 1.4947\n",
      "epoch 0: avg loss: 1.6267995834350586\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Iteration</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Total Loss</td><td>▁</td></tr><tr><td>Train Loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Generated Text</td><td>I love yourself them...</td></tr><tr><td>Iteration</td><td>4500</td></tr><tr><td>Total Loss</td><td>1.6268</td></tr><tr><td>Train Loss</td><td>1.10765</td></tr><tr><td>Val Loss</td><td>1.49474</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">normal</strong> at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk</a><br/> View project at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_001252-attjdcbk/logs</code>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generated_text = generate_text(m, normal_encoding, \"I love\", 100, hyperparameters[\"device\"],\n",
    "                               hyperparameters['context_length'])\n",
    "generated_text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "WlN4AIGoMEbK",
    "outputId": "159c0162-571e-4057-acf6-382d8330efe9"
   },
   "id": "WlN4AIGoMEbK",
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"I love Harry's life; he was never.\\n\\nLADY CAPULET:\\nWitnom, thene on the bitted time-first of peace?\\n\\nNurse:\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "load_and_generate(model_path=\"./model0.pth\", encoding=normal_encoding, initial_text=\"I hate\", max_new_tokens=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "HONHDPvLVVDi",
    "outputId": "de2596e8-d3d5-4acb-9ba0-0a66a2106209"
   },
   "id": "HONHDPvLVVDi",
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-36-1910b91045ff>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I hate, hour! down them! Come, my ladom!\\n\\nMERCUTIO:\\nReady not amuse.\\n\\nHORTENSIA:\\nNot the silent read Barna'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 41
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hyperparameters = {\n",
    "    \"batch_size\": 8,\n",
    "    \"context_length\": 256,\n",
    "    \"max_iters\": 10,\n",
    "    \"eval_interval\": 500,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"eval_iters\": 200,\n",
    "    \"n_embed\": 384,\n",
    "    \"num_head\": 4,\n",
    "    \"num_layer\": 6,\n",
    "    \"dropout\": 0.2,\n",
    "    \"temperature\": 1.0,\n",
    "    \"epochs\": 1,\n",
    "    \"train_rate\": 0.9,\n",
    "    \"vocab_size\": sent_piece.get_vocab_size(),\n",
    "    \"steps\": 500,\n",
    "    \"bias\": False\n",
    "}\n",
    "\n",
    "hyperparameters[\"head_size\"] = hyperparameters[\"n_embed\"] // hyperparameters[\"num_head\"]"
   ],
   "metadata": {
    "id": "oVbdHHKj8dP9"
   },
   "id": "oVbdHHKj8dP9",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "f9c9d33b809232fa",
    "outputId": "858bc05d-0e3a-4cf9-b3fd-350585582248"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:jtm06l8q) before initializing another..."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sentencepiece</strong> at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/jtm06l8q' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/jtm06l8q</a><br/> View project at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_002429-jtm06l8q/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:jtm06l8q). Initializing new run:<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241216_002458-foonbolz</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/foonbolz' target=\"_blank\">sentencepiece</a></strong> to <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/foonbolz' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/foonbolz</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0: train loss 4.1760, val loss 4.1788\n",
      "step 500: train loss 1.8488, val loss 1.9655\n",
      "step 1000: train loss 1.5113, val loss 1.6999\n",
      "step 1500: train loss 1.3847, val loss 1.5913\n",
      "step 2000: train loss 1.3139, val loss 1.5444\n",
      "step 2500: train loss 1.2575, val loss 1.5163\n",
      "step 3000: train loss 1.2087, val loss 1.4950\n",
      "step 3500: train loss 1.1668, val loss 1.4887\n",
      "step 4000: train loss 1.1312, val loss 1.4730\n",
      "step 4500: train loss 1.0902, val loss 1.4947\n",
      "epoch 0: avg loss: 1.6089099645614624\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 44,
   "source": [
    "m2 = train_save(dataset_name=\"Dataset.txt\", encoding=\"sentencepiece\", parameters=hyperparameters, wandb_log=True)"
   ],
   "id": "f9c9d33b809232fa"
  },
  {
   "cell_type": "code",
   "source": [
    "load_and_generate(model_path=\"./model_sent1.pth\", encoding=sent_piece, initial_text=\"I Love\", max_new_tokens=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "odUvZ-lVhz0b",
    "outputId": "e995e138-2c4d-4056-8ad4-ecd249d2a280"
   },
   "id": "odUvZ-lVhz0b",
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-32-d5423ab24788>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I Love showersterate lambs unlawful gotten powder Meneazed stealtheldomills spread pratingded part cormorant Polixenesded accou sootheurel cause thousands shrill hangmen easesolation honeyIalleys glad cupbearer babe fools Florizelels wean sceptre innocentcame whet warrants cedar profanation eyeb arbitrateign fondly patron gulf appoint companions without inconstantfords familiar broke wayontempt Sebastian aidtwoulddoubledliar ali instructions Showif concludes ben pitchpolitmplespregnant disease chief hair habiliments mast thempt wrinkled proclaim eaten Cry Wr pursue dishonest kissBRpillars argument threat supposesnablehereby interrupteldomys steal'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 35
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "Pi2UuVMk9y6c"
   },
   "id": "Pi2UuVMk9y6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
