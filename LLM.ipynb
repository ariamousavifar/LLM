{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:47.740982Z",
     "start_time": "2024-12-13T18:43:47.726087Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9a7e8aa1b3814ff",
    "outputId": "f79ba14f-d83b-4a7a-be6f-2af7780f9c8e"
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import sentencepiece as spm\n",
    "import subprocess\n",
    "import wandb\n",
    "import nltk\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "f9a7e8aa1b3814ff",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:47.871439Z",
     "start_time": "2024-12-13T18:43:47.861997Z"
    },
    "id": "e67ac312e97d42f"
   },
   "cell_type": "code",
   "source": [
    "# !pip freeze > requirements.txt"
   ],
   "id": "e67ac312e97d42f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:47.943789Z",
     "start_time": "2024-12-13T18:43:47.940165Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bca6edd824b1b033",
    "outputId": "421ebbfa-168e-4cb4-834e-74a44008f46b"
   },
   "cell_type": "code",
   "source": "wandb.login()",
   "id": "bca6edd824b1b033",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.012871Z",
     "start_time": "2024-12-13T18:43:47.997947Z"
    },
    "id": "359f1802d1cc1390"
   },
   "cell_type": "code",
   "source": [
    "def check_requirements() -> bool:\n",
    "    try:\n",
    "        if not os.path.exists(\"requirements.txt\"):\n",
    "            raise FileNotFoundError(\"requirements.txt not found\")\n",
    "\n",
    "        result = subprocess.run(\n",
    "            [\"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "            check=True,  # Raise an exception if the command fails\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing requirements: {e.stderr}\")\n",
    "        return False"
   ],
   "id": "359f1802d1cc1390",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.099637Z",
     "start_time": "2024-12-13T18:43:48.095925Z"
    },
    "id": "37422fc4f98dd644"
   },
   "cell_type": "code",
   "source": [
    "# check_requirements()"
   ],
   "id": "37422fc4f98dd644",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.148524Z",
     "start_time": "2024-12-13T18:43:48.143489Z"
    },
    "id": "5ec024fa984c31af"
   },
   "cell_type": "code",
   "source": [
    "# # hyperparameters\n",
    "# batch_size = 32\n",
    "# block_size = 8\n",
    "# max_iters = 3000\n",
    "# eval_interval = 300\n",
    "# learning_rate = 1e-2\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embed = 32\n",
    "# # ------------"
   ],
   "id": "5ec024fa984c31af",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.200417Z",
     "start_time": "2024-12-13T18:43:48.196944Z"
    },
    "id": "fe02f43f8fac14da"
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(\"Dataset.txt\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "    os.rename(\"input.txt\", 'Dataset.txt')"
   ],
   "id": "fe02f43f8fac14da",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.231540Z",
     "start_time": "2024-12-13T18:43:48.205441Z"
    },
    "id": "7bba9bcbc5272509"
   },
   "cell_type": "code",
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: str, mode: str = \"normal\"):\n",
    "\n",
    "        self.tokens = set(nltk.word_tokenize(data))\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"normal\":\n",
    "            self.chars = sorted(set(train_text))  # get characters from the input data\n",
    "\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}  # map characters to integer indices\n",
    "            self.itos = {i: ch for i, ch in enumerate(self.chars)}  # map integer indices to characters\n",
    "            self.vocab_size = len(self.chars)\n",
    "\n",
    "        elif mode == \"sentencepiece\":\n",
    "            self.vocab_size = min(len(self.tokens), 10770)\n",
    "            spm.SentencePieceTrainer.train(model_prefix='shakespeare', input='Dataset.txt',\n",
    "                                           vocab_size=10770, unk_id=0, bos_id=1, eos_id=2, pad_id=3)\n",
    "\n",
    "\n",
    "        elif mode == \"tiktoken\":\n",
    "            self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "            self.vocab_size = self.enc.max_token_value + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.mode == \"normal\":\n",
    "            return [self.stoi[s] for s in text]\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.encode(text)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if self.mode == \"normal\":\n",
    "            return ''.join([self.itos[t] for t in tokens])\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.decode(tokens)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.decode(tokens)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "\n",
    "    # ```"
   ],
   "id": "7bba9bcbc5272509",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:48.254518Z",
     "start_time": "2024-12-13T18:43:48.234893Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa83771b2f7c2d6e",
    "outputId": "54cb762c-41e2-48fe-a8e2-741c3870647e"
   },
   "cell_type": "code",
   "source": [
    "with open(\"Dataset.txt\", \"r\") as file:\n",
    "    train_text = file.read()\n",
    "\n",
    "print(train_text[:500])"
   ],
   "id": "aa83771b2f7c2d6e",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:53.885925Z",
     "start_time": "2024-12-13T18:43:48.276236Z"
    },
    "id": "3c69851ade7e8691"
   },
   "cell_type": "code",
   "source": [
    "normal_encoding = CharDataset(train_text, mode=\"normal\")\n",
    "sent_piece = CharDataset(train_text, mode=\"sentencepiece\")\n",
    "tiktoken_encoding = CharDataset(train_text, mode=\"tiktoken\")"
   ],
   "id": "3c69851ade7e8691",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:54.558538Z",
     "start_time": "2024-12-13T18:43:53.887996Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dca0b7739b1d395",
    "outputId": "bfaf8ee3-56e3-4700-ff67-d4c67995e59f"
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Normal encoding: Length of sequence = {len(normal_encoding.encode(train_text))}, Vocab size = {normal_encoding.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"SentencePiece encoding: Length of sequence = {len(sent_piece.encode(train_text))}, Vocab size = {sent_piece.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"TikToken encoding: Length of sequence = {len(tiktoken_encoding.encode(train_text))}, Vocab size = {tiktoken_encoding.get_vocab_size()}\")"
   ],
   "id": "8dca0b7739b1d395",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal encoding: Length of sequence = 1115394, Vocab size = 65\n",
      "SentencePiece encoding: Length of sequence = 290364, Vocab size = 10770\n",
      "TikToken encoding: Length of sequence = 338025, Vocab size = 50257\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:54.734179Z",
     "start_time": "2024-12-13T18:43:54.561435Z"
    },
    "id": "a5ca390e5cf5a7fe"
   },
   "cell_type": "code",
   "source": [
    "# data = torch.tensor(normal_encoding.encode(train_text), dtype=torch.long)\n",
    "# print(data.shape, data.dtype)\n",
    "# print(data[:1000])"
   ],
   "id": "a5ca390e5cf5a7fe",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:55.057488Z",
     "start_time": "2024-12-13T18:43:54.736707Z"
    },
    "id": "378307723b7d6e7a"
   },
   "cell_type": "code",
   "source": [
    "# data2 = torch.tensor(sent_piece.encode(train_text), dtype=torch.long)\n",
    "# print(data2.shape, data2.dtype)\n",
    "# print(data2[:1000])"
   ],
   "id": "378307723b7d6e7a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:55.062713Z",
     "start_time": "2024-12-13T18:43:55.059059Z"
    },
    "id": "b42876778837d857"
   },
   "cell_type": "code",
   "source": [
    "# n = int(0.9 * len(data))\n",
    "\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]"
   ],
   "id": "b42876778837d857",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:55.067507Z",
     "start_time": "2024-12-13T18:43:55.063808Z"
    },
    "id": "f9a364fcef57da41"
   },
   "cell_type": "code",
   "source": [
    "# context_length = 8\n",
    "\n",
    "# print(train_data[:context_length])\n",
    "# print(normal_encoding.decode(train_data[:context_length].tolist()))"
   ],
   "id": "f9a364fcef57da41",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:55.090266Z",
     "start_time": "2024-12-13T18:43:55.069707Z"
    },
    "id": "82823b888661a6e8"
   },
   "cell_type": "code",
   "source": [
    "# train_data2 = data2[:n]\n",
    "# val_data2 = data2[n:]\n",
    "\n",
    "# print(train_data2[:context_length])\n",
    "# print(sent_piece.decode(train_data2[:context_length].tolist()))"
   ],
   "id": "82823b888661a6e8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:43:55.101893Z",
     "start_time": "2024-12-13T18:43:55.095935Z"
    },
    "id": "5aa32008cc25e3a1"
   },
   "cell_type": "code",
   "source": [
    "# x = train_data[:context_length]\n",
    "# y = train_data[1:context_length + 1]\n",
    "\n",
    "# for context in range(1, context_length):\n",
    "#     print(f\"context = {context}, input = {x[:context].tolist()}, target = {y[context - 1]}\")"
   ],
   "id": "5aa32008cc25e3a1",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T18:55:07.081913Z",
     "start_time": "2024-12-13T18:55:07.051799Z"
    },
    "id": "1cfbc1a9310f0e2c"
   },
   "cell_type": "code",
   "source": [
    "# xb, yb = get_batch(train_data)\n",
    "# print(\"input\")\n",
    "# print(xb.shape)\n",
    "# print(xb)\n",
    "# print(\"target\")\n",
    "# print(yb.shape)\n",
    "# print(yb)"
   ],
   "id": "1cfbc1a9310f0e2c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "def get_batch(data, context_length, batch_size, device):\n",
    "    start_idx = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[i: i + context_length] for i in start_idx])\n",
    "    y = torch.stack([data[i + 1: i + 1 + context_length] for i in start_idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "metadata": {
    "id": "Frbr5VJomsgj"
   },
   "id": "Frbr5VJomsgj",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:24.001576Z",
     "start_time": "2024-12-13T19:40:23.938891Z"
    },
    "id": "ad93f3d25ee36141"
   },
   "cell_type": "code",
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, context_length, n_embd, temperature, dropout, bias):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei / self.temperature, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ],
   "id": "ad93f3d25ee36141",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:24.337064Z",
     "start_time": "2024-12-13T19:40:24.330441Z"
    },
    "id": "927f800673ef1d3e"
   },
   "cell_type": "code",
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \"\"\" a multi-head attention layer \"\"\"\n",
    "\n",
    "    def __init__(self, num_head, head_size, context_length, n_embd, temperature, dropout, bias):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size, context_length=context_length, n_embd=n_embd,\n",
    "                                         temperature=temperature, dropout=dropout, bias=bias) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        return self.dropout(self.proj(out))"
   ],
   "id": "927f800673ef1d3e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:24.891698Z",
     "start_time": "2024-12-13T19:40:24.886757Z"
    },
    "id": "e40ea3153f88a992"
   },
   "cell_type": "code",
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "e40ea3153f88a992",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:25.549310Z",
     "start_time": "2024-12-13T19:40:25.544271Z"
    },
    "id": "e73bcc4f6c617ba8"
   },
   "cell_type": "code",
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    # num_head, head_size, context_length, n_embd, temperature, dropout, bias):\n",
    "    def __init__(self, num_head, context_length, n_embd, temperature, dropout, bias):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_head\n",
    "        self.sa = MultiHead(num_head, head_size, context_length, n_embd, temperature, dropout, bias)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ],
   "id": "e73bcc4f6c617ba8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:40:26.604470Z",
     "start_time": "2024-12-13T19:40:26.561595Z"
    },
    "id": "b7ccc59875cc7aee"
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm1d:  # (used to be BatchNorm1d)\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True)  # batch mean\n",
    "        xvar = x.var(1, keepdim=True)  # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ],
   "id": "b7ccc59875cc7aee",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:49:07.879915Z",
     "start_time": "2024-12-13T19:49:07.767627Z"
    },
    "id": "f8a41f136360200c"
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLangModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_layer, num_head=8, head_size=16, context_length=8, n_embed=32, temperature=1.0,\n",
    "                 dropout=0.0,\n",
    "                 bias=False):\n",
    "        # print all parameters:\n",
    "        # print(\"THIS IS MODEL\")\n",
    "        # print(f\"vocab_size = {vocab_size}, num_layer = {num_layer}, num_head = {num_head}, head_size = {head_size}\")\n",
    "        # print(f\"context_length = {context_length}, n_embed = {n_embed}, temperature = {temperature}, dropout = {dropout}\")\n",
    "        # print(f\"bias = {bias}\")\n",
    "        # print(\"_____________________\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position = nn.Embedding(context_length, n_embed)\n",
    "\n",
    "        # self.self_attention_head = MultiHead(num_head=num_head, head_size=head_size, context_length=context_length,\n",
    "        #                                      n_embd=n_embed, temperature=temperature, dropout=dropout, bias=bias)\n",
    "        self.blocks = nn.Sequential(*[Block(num_head=num_head, context_length=context_length, n_embd=n_embed,\n",
    "                                            temperature=temperature, dropout=dropout, bias=bias) for _ in\n",
    "                                      range(num_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.feedforward = FeedFoward(n_embed, dropout)\n",
    "        self.langhead = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # T: sequence length (number of tokens) , B: batch size (number of sequences)\n",
    "        B, T = indices.shape\n",
    "        tok_embeds = self.token_embedding(indices)  # (B, T, n_embed)\n",
    "        pos_embeds = self.position(torch.arange(T, device=indices.device))  # (T, n_embed)\n",
    "        x = tok_embeds + pos_embeds  # (B, T, n_embed)\n",
    "        # x = self.self_attention_head(x)  # (B, T, n_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.feedforward(x)  # (B, T, n_embed)\n",
    "        # logits = self.langhead(self.token_embedding(indices))  # (B, T, vocab_size)\n",
    "        logits = self.langhead(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, init_token, max_new_tokens, context_length):\n",
    "        sequence = init_token\n",
    "        for itr in range(max_new_tokens):\n",
    "            sequence_cropped = sequence[:, -context_length:]\n",
    "            logits, loss = self(sequence_cropped)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # next_token = torch.argmax(probs, dim=-1)\n",
    "            # next_token = next_token.unsqueeze(1)\n",
    "            sequence = torch.cat((sequence, next_token), dim=1)\n",
    "        return sequence"
   ],
   "id": "f8a41f136360200c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:49:08.282501Z",
     "start_time": "2024-12-13T19:49:08.278630Z"
    },
    "id": "4651546740d68320"
   },
   "cell_type": "code",
   "source": [
    "# # hyperparameters\n",
    "\n",
    "# batch_size = 64\n",
    "# context_length = 256\n",
    "# max_iters = 5000\n",
    "# eval_interval = 500\n",
    "# learning_rate = 3e-4\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embed = 384\n",
    "# num_head = 4\n",
    "# n_layer = 6\n",
    "# dropout = 0.2\n",
    "# temperature = 1.0\n",
    "# epochs = 1\n",
    "\n",
    "# head_size = n_embed // num_head"
   ],
   "id": "4651546740d68320",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# print(f\"max_iters: {max_iters}, epochs: {epochs}, steps: {steps}, eval_interval: {eval_interval}\")\n",
    "# print(f\"learning_rate: {learning_rate}, device: {device}, eval_iters: {eval_iters}\")\n",
    "# print(f\"n_embed: {n_embed}, num_head: {num_head}, num_layer: {n_layer}, dropout: {dropout}\")\n",
    "# print(f\"temperature: {temperature}, context_length: {context_length}, vocab_size: {normal_encoding.get_vocab_size()}\")\n",
    "# print(f\"head_size: {head_size}, barch_size = {batch_size}, train_rate ={0.9}\" )"
   ],
   "metadata": {
    "id": "jRu1yCKGyw6j"
   },
   "id": "jRu1yCKGyw6j",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:49:15.065388Z",
     "start_time": "2024-12-13T19:49:09.185635Z"
    },
    "id": "714c9e89761a1913"
   },
   "cell_type": "code",
   "source": [
    "# model = BigramLangModel(vocab_size=normal_encoding.get_vocab_size(), num_layer=n_layer, n_embed=n_embed,\n",
    "#                         context_length=context_length,\n",
    "#                         temperature=temperature, dropout=dropout, num_head=4, head_size=head_size)\n",
    "\n",
    "# m = model.to(device)\n",
    "# logits, loss = m(indices=xb, targets=yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "# initial_token = torch.tensor(normal_encoding.encode('\\n'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "# # 0 == new line char\n",
    "\n",
    "# generated_text = normal_encoding.decode(m.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())\n",
    "\n",
    "# print(f\"Generated Sequence : {generated_text}\")"
   ],
   "id": "714c9e89761a1913",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:50:17.499759Z",
     "start_time": "2024-12-13T19:49:55.790485Z"
    },
    "id": "c6758d9f31d26673"
   },
   "cell_type": "code",
   "source": [
    "# torch.manual_seed(1337)\n",
    "# xb2, yb2 = get_batch(train_data2)\n",
    "# model2 = BigramLangModel(vocab_size=sent_piece.get_vocab_size(), num_layer=n_layer, n_embed=n_embed,\n",
    "#                          context_length=context_length, temperature=temperature, dropout=dropout, num_head=4,\n",
    "#                          head_size=head_size)\n",
    "# m2 = model2.to(device)\n",
    "# logits, loss = m2(xb2, yb2)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "# initial_token = torch.tensor(sent_piece.encode('I Love You'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "# generated_text = sent_piece.decode(m2.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())\n",
    "\n",
    "# print(f\"Generated Sequence : {generated_text}\")"
   ],
   "id": "c6758d9f31d26673",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:50:22.393172Z",
     "start_time": "2024-12-13T19:50:22.363418Z"
    },
    "id": "6fd4d1ae6090ee90"
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, eval_iters, context_length, batch_size, device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for data in [train_data, val_data]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data, context_length, batch_size, device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out['train' if data is train_data else 'val'] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "6fd4d1ae6090ee90",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T19:50:24.013007Z",
     "start_time": "2024-12-13T19:50:23.999081Z"
    },
    "id": "321f466874bcd788"
   },
   "cell_type": "code",
   "source": [
    "def train(model, data, val_data, context_length, batch_size, device, max_iters=5000, epochs=10, steps=100,\n",
    "          eval_iters=200, eval_interval=100, learning_rate=1e-3, wandb_log=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for iter in range(max_iters):\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if iter % eval_interval == 0:\n",
    "                losses = estimate_loss(model, data, val_data, eval_iters, context_length, batch_size, device)\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                total_loss += losses['train']\n",
    "\n",
    "                if wandb_log:\n",
    "                    wandb.log({\"Iteration\": iter, \"Train Loss\": losses['train'], \"Val Loss\": losses['val']})\n",
    "\n",
    "            xb, yb = get_batch(data, context_length, batch_size, device)\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"epoch {epoch}: avg loss: {total_loss * eval_interval / max_iters}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\"Epoch\": epoch + 1, \"Total Loss\": total_loss * eval_interval / max_iters})\n"
   ],
   "id": "321f466874bcd788",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_text(model, encoding, initial_text: str, max_new_tokens: int, device, context_length: int) -> str:\n",
    "    initial_token = torch.tensor(encoding.encode(initial_text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    generated_text = encoding.decode(\n",
    "        model.generate(context_length=context_length, init_token=initial_token, max_new_tokens=max_new_tokens)[\n",
    "            0].tolist())\n",
    "    return generated_text"
   ],
   "metadata": {
    "id": "NKVwj9W-HLHy"
   },
   "id": "NKVwj9W-HLHy",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def save_model(model, encoding, parameters):\n",
    "    i = 0\n",
    "    path = f\"./model{i}.pth\"\n",
    "    while os.path.exists(path):\n",
    "        i += 1\n",
    "        path = f\"./model{i}.pth\"\n",
    "\n",
    "    torch.save(\n",
    "        dict(\n",
    "            model=model.state_dict(),\n",
    "            encoding=encoding,\n",
    "            parameters=parameters\n",
    "        ), path)"
   ],
   "metadata": {
    "id": "wWDD82QKLLwo"
   },
   "id": "wWDD82QKLLwo",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_save(dataset_name: str, encoding: str, parameters: dict, wandb_log=False):\n",
    "    global s\n",
    "    with open(dataset_name, \"r\") as file:\n",
    "        train_text = file.read()\n",
    "\n",
    "    encoding_name = encoding\n",
    "    encoding = CharDataset(train_text, mode=encoding)\n",
    "\n",
    "    data = torch.tensor(normal_encoding.encode(train_text), dtype=torch.long)\n",
    "\n",
    "    n = int(parameters[\"train_rate\"] * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    xb, yb = get_batch(train_data, context_length=parameters['context_length'], batch_size=parameters['batch_size'],\n",
    "                       device=parameters[\"device\"])\n",
    "\n",
    "    model = BigramLangModel(parameters[\"vocab_size\"], parameters[\"num_layer\"], n_embed=parameters[\"n_embed\"],\n",
    "                            context_length=parameters[\"context_length\"], temperature=parameters[\"temperature\"],\n",
    "                            dropout=parameters[\"dropout\"], num_head=parameters[\"num_head\"],\n",
    "                            head_size=parameters[\"head_size\"])\n",
    "\n",
    "    m = model.to(parameters[\"device\"])\n",
    "\n",
    "    logits, loss = m(indices=xb, targets=yb)\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.init(\n",
    "            project=\"LLM\",\n",
    "            config={\n",
    "                \"learning_rate\": parameters[\"learning_rate\"],\n",
    "                \"architecture\": \"Transformers\",\n",
    "                \"dataset\": \"Shakespeare\",\n",
    "            },\n",
    "\n",
    "            name=encoding_name\n",
    "        )\n",
    "\n",
    "    train(model=m, data=train_data, val_data=val_data, context_length=parameters['context_length'],\n",
    "          batch_size=parameters['batch_size'], device=parameters[\"device\"], learning_rate=parameters[\"learning_rate\"],\n",
    "          max_iters=parameters[\"max_iters\"],\n",
    "          epochs=parameters[\"epochs\"], steps=parameters[\"steps\"], eval_interval=parameters[\"eval_interval\"],\n",
    "          wandb_log=wandb_log)\n",
    "    # return m\n",
    "    generated_text = generate_text(m, encoding, \"I love\", 100, parameters[\"device\"], parameters['context_length'])\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.log({\"Generated Text\": generated_text})\n",
    "        wandb.finish()\n",
    "\n",
    "    save_model(m, encoding, parameters)\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "id": "lQ-lsPUF1ehA"
   },
   "id": "lQ-lsPUF1ehA",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_model(path):\n",
    "    if os.path.exists(path):\n",
    "        # Explicitly map the model to the CPU\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "        hyperparameters = checkpoint[\"parameters\"]\n",
    "        # Use the correct number of heads from the saved model\n",
    "        num_head = hyperparameters[\"num_head\"]\n",
    "        # Calculate head_size based on num_head\n",
    "        head_size = hyperparameters[\"n_embed\"] // num_head\n",
    "        model = BigramLangModel(\n",
    "            vocab_size=hyperparameters[\"vocab_size\"],\n",
    "            num_layer=hyperparameters[\"num_layer\"],\n",
    "            n_embed=hyperparameters[\"n_embed\"],\n",
    "            context_length=hyperparameters[\"context_length\"],\n",
    "            temperature=hyperparameters[\"temperature\"],\n",
    "            dropout=hyperparameters[\"dropout\"],\n",
    "            num_head=hyperparameters[\"num_head\"],\n",
    "            head_size=hyperparameters[\"head_size\"]\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        # Return the model and hyperparameters as separate elements.\n",
    "        return model, hyperparameters\n",
    "    else:\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "        return None, None  # Return None for both when file not found."
   ],
   "metadata": {
    "id": "OhgvQ2MyV4jj"
   },
   "id": "OhgvQ2MyV4jj",
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_and_generate(model_path: str, encoding, initial_text: str, max_new_tokens: int):\n",
    "    if not load_model(model_path):\n",
    "        print(\"Model not found\")\n",
    "        return\n",
    "\n",
    "    model, parameters = load_model(model_path)\n",
    "\n",
    "    context_length = parameters[\"context_length\"]\n",
    "    # vocab_size = encoding.get_vocab_size()\n",
    "\n",
    "    # # Correct the vocab_size in the parameters dictionary:\n",
    "    # parameters[\"vocab_size\"] = vocab_size\n",
    "\n",
    "    # # Ensure model is using the correct vocabulary size:\n",
    "    # model.token_embedding = nn.Embedding(vocab_size, model.n_embed)\n",
    "    # model.langhead = nn.Linear(model.n_embed, vocab_size)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    generated_text = generate_text(model, encoding, initial_text, max_new_tokens, device, context_length)\n",
    "    return generated_text"
   ],
   "metadata": {
    "id": "qmoRznHwZTEm"
   },
   "id": "qmoRznHwZTEm",
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "hyperparameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"context_length\": 256,\n",
    "    \"max_iters\": 5000,\n",
    "    \"eval_interval\": 500,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"eval_iters\": 200,\n",
    "    \"n_embed\": 384,\n",
    "    \"num_head\": 4,\n",
    "    \"num_layer\": 6,\n",
    "    \"dropout\": 0.2,\n",
    "    \"temperature\": 1.0,\n",
    "    \"epochs\": 1,\n",
    "    \"train_rate\": 0.9,\n",
    "    \"vocab_size\": normal_encoding.get_vocab_size(),\n",
    "    \"steps\": 500,\n",
    "    \"bias\": False\n",
    "}\n",
    "\n",
    "hyperparameters[\"head_size\"] = hyperparameters[\"n_embed\"] // hyperparameters[\"num_head\"]"
   ],
   "metadata": {
    "id": "OqtHi0-hdiDL"
   },
   "id": "OqtHi0-hdiDL",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "m = train_save(dataset_name=\"Dataset.txt\", encoding=\"normal\", parameters=hyperparameters, wandb_log=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "naWEIW6N1itW",
    "outputId": "14cffd8f-911f-44ab-9b7b-0ebef61eca83"
   },
   "id": "naWEIW6N1itW",
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mariamosavefar\u001B[0m (\u001B[33mariamosavefar-universit-de-gen-ve\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241216_001252-attjdcbk</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk' target=\"_blank\">normal</a></strong> to <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0: train loss 4.2228, val loss 4.2235\n",
      "step 500: train loss 1.8823, val loss 1.9928\n",
      "step 1000: train loss 1.5255, val loss 1.7086\n",
      "step 1500: train loss 1.3978, val loss 1.6077\n",
      "step 2000: train loss 1.3216, val loss 1.5537\n",
      "step 2500: train loss 1.2686, val loss 1.5147\n",
      "step 3000: train loss 1.2196, val loss 1.5030\n",
      "step 3500: train loss 1.1805, val loss 1.4883\n",
      "step 4000: train loss 1.1417, val loss 1.4980\n",
      "step 4500: train loss 1.1076, val loss 1.4947\n",
      "epoch 0: avg loss: 1.6267995834350586\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Iteration</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Total Loss</td><td>▁</td></tr><tr><td>Train Loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Generated Text</td><td>I love yourself them...</td></tr><tr><td>Iteration</td><td>4500</td></tr><tr><td>Total Loss</td><td>1.6268</td></tr><tr><td>Train Loss</td><td>1.10765</td></tr><tr><td>Val Loss</td><td>1.49474</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">normal</strong> at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/attjdcbk</a><br/> View project at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_001252-attjdcbk/logs</code>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generated_text = generate_text(m, normal_encoding, \"I love\", 100, hyperparameters[\"device\"],\n",
    "                               hyperparameters['context_length'])\n",
    "generated_text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "WlN4AIGoMEbK",
    "outputId": "159c0162-571e-4057-acf6-382d8330efe9"
   },
   "id": "WlN4AIGoMEbK",
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"I love Harry's life; he was never.\\n\\nLADY CAPULET:\\nWitnom, thene on the bitted time-first of peace?\\n\\nNurse:\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "load_and_generate(model_path=\"./model0.pth\", encoding=normal_encoding, initial_text=\"I hate\", max_new_tokens=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "HONHDPvLVVDi",
    "outputId": "de2596e8-d3d5-4acb-9ba0-0a66a2106209"
   },
   "id": "HONHDPvLVVDi",
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-36-1910b91045ff>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I hate, hour! down them! Come, my ladom!\\n\\nMERCUTIO:\\nReady not amuse.\\n\\nHORTENSIA:\\nNot the silent read Barna'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 41
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hyperparameters = {\n",
    "    \"batch_size\": 8,\n",
    "    \"context_length\": 256,\n",
    "    \"max_iters\": 10,\n",
    "    \"eval_interval\": 500,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"eval_iters\": 200,\n",
    "    \"n_embed\": 384,\n",
    "    \"num_head\": 4,\n",
    "    \"num_layer\": 6,\n",
    "    \"dropout\": 0.2,\n",
    "    \"temperature\": 1.0,\n",
    "    \"epochs\": 1,\n",
    "    \"train_rate\": 0.9,\n",
    "    \"vocab_size\": sent_piece.get_vocab_size(),\n",
    "    \"steps\": 500,\n",
    "    \"bias\": False\n",
    "}\n",
    "\n",
    "hyperparameters[\"head_size\"] = hyperparameters[\"n_embed\"] // hyperparameters[\"num_head\"]"
   ],
   "metadata": {
    "id": "oVbdHHKj8dP9"
   },
   "id": "oVbdHHKj8dP9",
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "m2 = train_save(dataset_name=\"Dataset.txt\", encoding=\"sentencepiece\", parameters=hyperparameters, wandb_log=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRGojobk8_eZ",
    "outputId": "0afe0afb-3708-4283-b9a3-bc8061e24f9d"
   },
   "id": "rRGojobk8_eZ",
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0: train loss 9.2610, val loss 9.2661\n",
      "epoch 0: avg loss: 463.05029296875\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "f9c9d33b809232fa",
    "outputId": "858bc05d-0e3a-4cf9-b3fd-350585582248"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:jtm06l8q) before initializing another..."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sentencepiece</strong> at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/jtm06l8q' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/jtm06l8q</a><br/> View project at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_002429-jtm06l8q/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:jtm06l8q). Initializing new run:<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241216_002458-foonbolz</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/foonbolz' target=\"_blank\">sentencepiece</a></strong> to <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/foonbolz' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/foonbolz</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0: train loss 4.1760, val loss 4.1788\n",
      "step 500: train loss 1.8488, val loss 1.9655\n",
      "step 1000: train loss 1.5113, val loss 1.6999\n",
      "step 1500: train loss 1.3847, val loss 1.5913\n",
      "step 2000: train loss 1.3139, val loss 1.5444\n",
      "step 2500: train loss 1.2575, val loss 1.5163\n",
      "step 3000: train loss 1.2087, val loss 1.4950\n",
      "step 3500: train loss 1.1668, val loss 1.4887\n",
      "step 4000: train loss 1.1312, val loss 1.4730\n",
      "step 4500: train loss 1.0902, val loss 1.4947\n",
      "epoch 0: avg loss: 1.6089099645614624\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 44,
   "source": [
    "m2 = train_save(dataset_name=\"Dataset.txt\", encoding=\"sentencepiece\", parameters=hyperparameters, wandb_log=True)"
   ],
   "id": "f9c9d33b809232fa"
  },
  {
   "cell_type": "code",
   "source": [
    "load_and_generate(model_path=\"./model_sent1.pth\", encoding=sent_piece, initial_text=\"I Love\", max_new_tokens=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "odUvZ-lVhz0b",
    "outputId": "e995e138-2c4d-4056-8ad4-ecd249d2a280"
   },
   "id": "odUvZ-lVhz0b",
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-32-d5423ab24788>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I Love showersterate lambs unlawful gotten powder Meneazed stealtheldomills spread pratingded part cormorant Polixenesded accou sootheurel cause thousands shrill hangmen easesolation honeyIalleys glad cupbearer babe fools Florizelels wean sceptre innocentcame whet warrants cedar profanation eyeb arbitrateign fondly patron gulf appoint companions without inconstantfords familiar broke wayontempt Sebastian aidtwoulddoubledliar ali instructions Showif concludes ben pitchpolitmplespregnant disease chief hair habiliments mast thempt wrinkled proclaim eaten Cry Wr pursue dishonest kissBRpillars argument threat supposesnablehereby interrupteldomys steal'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 35
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "Pi2UuVMk9y6c"
   },
   "id": "Pi2UuVMk9y6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
