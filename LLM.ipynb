{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:23.065894Z",
     "start_time": "2024-12-12T04:55:15.781129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import sentencepiece as spm\n",
    "import subprocess\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "f9a7e8aa1b3814ff",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:23.070787Z",
     "start_time": "2024-12-12T04:55:23.068075Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip freeze > requirements.txt",
   "id": "e67ac312e97d42f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ],
   "id": "1e7b3b30f73d013f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:23.081299Z",
     "start_time": "2024-12-12T04:55:23.074188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_requirements() -> bool:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "            check=True,  # Raise an exception if the command fails\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        print(result.stdout)  # Optional: Print installation output\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing requirements: {e.stderr}\")\n",
    "        return False\n"
   ],
   "id": "359f1802d1cc1390",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:23.087454Z",
     "start_time": "2024-12-12T04:55:23.084450Z"
    }
   },
   "cell_type": "code",
   "source": "# check_requirements()",
   "id": "37422fc4f98dd644",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:23.093466Z",
     "start_time": "2024-12-12T04:55:23.089720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(\"Dataset.txt\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "    os.rename(\"input.txt\", 'Dataset.txt')"
   ],
   "id": "fe02f43f8fac14da",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:23.106816Z",
     "start_time": "2024-12-12T04:55:23.095679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: str, mode: str = \"normal\"):\n",
    "\n",
    "        self.tokens = set(nltk.word_tokenize(data))\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"normal\":\n",
    "            self.chars = sorted(set(train_text))  # get characters from the input data\n",
    "\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}  # map characters to integer indices\n",
    "            self.itos = {i: ch for i, ch in enumerate(self.chars)}  # map integer indices to characters\n",
    "            self.vocab_size = len(self.chars)\n",
    "\n",
    "        elif mode == \"sentencepiece\":\n",
    "            self.vocab_size = min(len(self.tokens), 10770)\n",
    "            spm.SentencePieceTrainer.train(model_prefix='shakespeare', input='Dataset.txt',\n",
    "                                           vocab_size=10770, unk_id=0, bos_id=1, eos_id=2, pad_id=3)\n",
    "\n",
    "\n",
    "        elif mode == \"tiktoken\":\n",
    "            self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "            self.vocab_size = self.enc.max_token_value + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.mode == \"normal\":\n",
    "            return [self.stoi[s] for s in text]\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.encode(text)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if self.mode == \"normal\":\n",
    "            return ''.join([self.itos[t] for t in tokens])\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.decode(tokens)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.decode(tokens)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "\n",
    "    # ```"
   ],
   "id": "7bba9bcbc5272509",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:23.120360Z",
     "start_time": "2024-12-12T04:55:23.111653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"Dataset.txt\", \"r\") as file:\n",
    "    train_text = file.read()\n",
    "\n",
    "print(train_text[:500])"
   ],
   "id": "aa83771b2f7c2d6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:32.844269Z",
     "start_time": "2024-12-12T04:55:23.125641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_dataset1 = CharDataset(train_text, mode=\"normal\")\n",
    "char_dataset2 = CharDataset(train_text, mode=\"sentencepiece\")\n",
    "char_dataset3 = CharDataset(train_text, mode=\"tiktoken\")"
   ],
   "id": "3c69851ade7e8691",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:33.982442Z",
     "start_time": "2024-12-12T04:55:32.848251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Normal encoding: Length of sequence = {len(char_dataset1.encode(train_text))}, Vocab size = {char_dataset1.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"SentencePiece encoding: Length of sequence = {len(char_dataset2.encode(train_text))}, Vocab size = {char_dataset2.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"TikToken encoding: Length of sequence = {len(char_dataset3.encode(train_text))}, Vocab size = {char_dataset3.get_vocab_size()}\")"
   ],
   "id": "8dca0b7739b1d395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal encoding: Length of sequence = 1115394, Vocab size = 65\n",
      "SentencePiece encoding: Length of sequence = 290364, Vocab size = 10770\n",
      "TikToken encoding: Length of sequence = 338025, Vocab size = 50257\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:34.275311Z",
     "start_time": "2024-12-12T04:55:33.984478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor(char_dataset1.encode(train_text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ],
   "id": "a5ca390e5cf5a7fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:34.689321Z",
     "start_time": "2024-12-12T04:55:34.277689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data2 = torch.tensor(char_dataset2.encode(train_text), dtype=torch.long)\n",
    "print(data2.shape, data2.dtype)\n",
    "print(data2[:1000])"
   ],
   "id": "378307723b7d6e7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([290364]) torch.int64\n",
      "tensor([  160,   346,     5,  1001,    54,  1671,   208,   953,     4,   181,\n",
      "           27,   147,     6,   421,     5,   997,     4,   147,     6,   160,\n",
      "          346,     5,   112,    58,    47,  1968,   540,    10,   292,   117,\n",
      "           10,  4422,    19,   421,     5,  7385,     6,  1968,     6,   160,\n",
      "          346,     5,   160,     4,    15,   109,  1602,   479,    26,  1795,\n",
      "          785,    10,     7,   397,     6,   421,     5,   184,   109,     8,\n",
      "           72,     4,    54,   109,     8,    72,     6,   160,   346,     5,\n",
      "          248,    96,   461,    37,     4,    11,    54,     8,    65,    34,\n",
      "         1763,    78,    59,   227,  3029,     6,   244,     8,    72,    16,\n",
      "         7476,    19,   421,     5,   165,    73,  4082,    64,     8,    72,\n",
      "           13,   107,    29,    28,   230,     5,   293,     4,   293,    21,\n",
      "           92,   282,   346,     5,   727,   314,     4,    68,  1339,     6,\n",
      "          160,   346,     5,   184,    58,  7523,   289,  1339,     4,     7,\n",
      "         2571,    68,     6,    69,  1818,  7691,    64,    84,  4491,    96,\n",
      "            5,    74,    89,    84,   855,    96,    44,     7,  6933,     4,\n",
      "          626,    29,   114,  2357,     4,    54,   384,  1376,    89,  4491,\n",
      "           24,    96,  5998,   111,    13,    44,    89,   187,    54,    58,\n",
      "          143,   419,     5,     7,  5515,    22,  2970,    12,    96,     4,\n",
      "            7,  2806,    14,    59,  2033,     4,    26,    40,   126, 10239,\n",
      "         2700,    10,  2025,  8022,    86,  5213,    13,    59,  3862,    26,\n",
      "           16,  1784,    10,    83,   248,    96,   776,    36,    30,    59,\n",
      "         5599,     4,   467,    54,   903,    18,  7729,     5,    32,     7,\n",
      "          599,   109,     9,   147,    36,    20,  4617,    32,  3111,     4,\n",
      "           25,    20,  4328,    32,   776,     6,    92,   282,   346,     5,\n",
      "          523,    15,  1671,  1232,  4262,   334,  1602,   479,    19,   421,\n",
      "            5,    66,   689,    37,   365,     5,    35,     8,    12,    16,\n",
      "          238,  1261,    10,     7,   714,  7982,     6,    92,   282,   346,\n",
      "            5,   120,  4766,    15,    63,  2123,    35,   530,   230,    32,\n",
      "           33,   620,    19,   160,   346,     5,  2315,   141,   101,    13,\n",
      "           11,   388,    28,   743,    10,   162,    37,    68,   734,    32,\n",
      "           72,     4,    44,    22,    35,  1087,    12,   353,    30,   262,\n",
      "          580,     6,    92,   282,   346,     5,   323,     4,    44,   147,\n",
      "           25,  6050,   111,     6,   160,   346,     5,     9,    95,   361,\n",
      "           15,     4,    63,    35,   106,   230,  7337,     4,    35,   137,\n",
      "           29,    10,    22,   524,     5,   372,  1093,    39,  8907,    24,\n",
      "          204,   167,    28,   743,    10,    95,    29,    81,    32,    33,\n",
      "          620,    35,   137,    29,    10,   490,    33,   283,    11,    10,\n",
      "           28,   363,   111,   580,    13,   129,    35,    26,     4,   529,\n",
      "          371,     7,    16,  9775,    14,    33,   945,     6,    92,   282,\n",
      "          346,     5,    69,    35,   214,   458,    20,    33,   588,     4,\n",
      "           15,  1527,    16,  1519,    20,    37,     6,   112,   116,    20,\n",
      "           61,   251,    95,    35,    26,  5537,   491,     6,   160,   346,\n",
      "            5,   132,     9,   116,    25,     4,     9,   442,    25,    28,\n",
      "         2562,    14,  5872,    13,    35,   106,  1126,     4,    30,  6751,\n",
      "            4,    10,  4031,    20,  4386,     6,    69,  5748,    58,   163,\n",
      "           19,    45,   270,   861,   201,     8,     7,   617,    26,  1398,\n",
      "          153,     5,   415,   310,    54,  3820,    91,    19,    10,     7,\n",
      "         1683,    21,   421,     5,   177,     4,    98,     6,   160,   346,\n",
      "            5,  3142,    21,   236,   308,    91,    19,    92,   282,   346,\n",
      "            5,  2265,   141,  1684,  8775,  7840,   644,    13,   103,    22,\n",
      "          106,  1625,  1100,     7,   397,     6,   160,   346,     5,   127,\n",
      "            8,    12,   103,   864,   576,     5,    84,    47,     7,   355,\n",
      "          114,    41,    21,    18,   303,   267,   100,     5,    69,   832,\n",
      "            8,    12,     4,    17,  2796,     4,    20,   217,    19,   215,\n",
      "          131,    15,   152,  5745,    11,  7326,    19,    45,   550,    19,\n",
      "          147,     4,     9,   307,    15,     6,   160,   346,     5,   456,\n",
      "          546,    26,    25,  2193,    10,     7,  1608,    13,    89,    34,\n",
      "          128,    20,  9781,    36,    32,    72,   890,    63,    54,  1876,\n",
      "           10,    57,     4,   129,    75,    54,     8,    65,   431,    52,\n",
      "         1057,    20,  1082,     6,   430,    95,   289,  3152,    34,   975,\n",
      "         6245,     5,    89,    55,   109,    54,    34,   975,   568,   143,\n",
      "            6,    18,   303,   267,   100,     5,   156,     4,  1489,     4,\n",
      "           17,    68,   305,     4,   154,   864,  3563,     4,   396,    15,\n",
      "         3589,  1333,    19,   160,   346,     5,   184,   214,     4,    79,\n",
      "            4,    54,    58,  1811,   979,     6,    18,   303,   267,   100,\n",
      "            5,     9,   180,    15,     4,   305,     4,   188,  6376,   946,\n",
      "          692,   370,     7,  2571,    14,    15,     6,    77,    31,  2273,\n",
      "            4,   207,  4019,    20,    36,  4056,     4,    15,   110,    40,\n",
      "          101,  2983,    78,     7,   274,    30,    31,  8351,  6336,    40,\n",
      "         2841,    83,    66,   689,     7,  1084,   457,     4,   440,   862,\n",
      "           42,    64,    45,   251,    29,  3592,     4,  1877,    51,   896,\n",
      "          484,  5711,   175,    73,   975,  4546,  7299,   117,   167,   298,\n",
      "         3360,    20,    31,  4371,     6,    77,     7,  4056,     4,    45,\n",
      "          599,     4,    25,     7,  2571,     4,   108,    29,     4,    11,\n",
      "          207,  1345,    10,    83,     4,    25,   568,     4,   116,   458,\n",
      "            6,  1481,     4,   112,    58,  4562,    53,  3756,  1032,  1310,\n",
      "          215,    73,   965,    12,    15,     4,    11,    15,  1166,    45,\n",
      "         5852,   201,     8,     7,   457,     4,   236,   692,    32,    15,\n",
      "          122,  2876,     4,   234,    15,   850,    83,    40,   900,     6,\n",
      "          160,   346,     5,   120,  3191,    32,    96,    21,  1557,     4,\n",
      "          516,    21,   430,   635,     8,    80,   692,    24,    32,    96,\n",
      "          142,     5,  1104,    96,    10,  4422,     4,    11,    86,  2160,\n",
      "           39, 10413,  3207,  8923,    30,  2710,    13,   108,  5297,    12,\n",
      "           32,    96,  8069,     4,    10,  3523,  4601,    12,    13,  2535,\n",
      "         2353,   208,  2357,  1267,  5239,    88,   334,     7,   895,     4,\n",
      "           11,  2957,    73,  3265,    51,  3915,  2353,     4,    10,  5141,\n",
      "          144,    11,  3408,     7,   289,     6,   132,     7,  1069,  1440,\n",
      "           96,    25,   144,     4,    89,    42,    13,    11,   125,     8,\n",
      "           12,    47,     7,    93,    89,   294,    96,     6,    18,   303,\n",
      "          267,   100,     5,   347,  1310,    15,   116,  3747,  1333,  2744,\n",
      "         6050,     4,   212,    28,  2419,    14,  2041,     6,     9,    55,\n",
      "          180,    15,    66,  1140,   858,     5,    29,   110,    28,    15,\n",
      "           34,   455,    29,    13,    67,     4,   447,    29,  2278,    17,\n",
      "          696,     4,     9,    42,  3876,    46,  3050,    52,    72,    16,\n",
      "          336,    73,     6,   160,   346,     5,   319,     4,     9,     8,\n",
      "           65,   181,    29,     4,    79,     5,   142,    15,   116,    25,\n",
      "          187,    10,  2111,  2257,   332,    59,  2507,    30,    16,   858,\n",
      "            5,    44,     4,   126,    52,    72,   490,    15,     4,   881,\n",
      "            6,    18,   303,   267,   100,     5,   342,    81,    16,   155,\n",
      "          149,    47,     7,   508,     8,    12,  3443,  7126,     8,    24])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:34.694434Z",
     "start_time": "2024-12-12T04:55:34.691268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "b42876778837d857",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:34.700825Z",
     "start_time": "2024-12-12T04:55:34.696528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = 8\n",
    "\n",
    "print(train_data[:context_length])\n",
    "print(char_dataset1.decode(train_data[:context_length].tolist()))"
   ],
   "id": "f9a364fcef57da41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "First Ci\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:34.725016Z",
     "start_time": "2024-12-12T04:55:34.702619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data2 = data2[:n]\n",
    "val_data2 = data2[n:]\n",
    "\n",
    "print(train_data2[:context_length])\n",
    "print(char_dataset2.decode(train_data2[:context_length].tolist()))"
   ],
   "id": "82823b888661a6e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 160,  346,    5, 1001,   54, 1671,  208,  953])\n",
      "First Citizen: Before we proceed any further\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:34.733873Z",
     "start_time": "2024-12-12T04:55:34.727079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length + 1]\n",
    "\n",
    "for context in range(1, context_length):\n",
    "    print(f\"context = {context}, input = {x[:context].tolist()}, target = {y[context - 1]}\")"
   ],
   "id": "5aa32008cc25e3a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context = 1, input = [18], target = 47\n",
      "context = 2, input = [18, 47], target = 56\n",
      "context = 3, input = [18, 47, 56], target = 57\n",
      "context = 4, input = [18, 47, 56, 57], target = 58\n",
      "context = 5, input = [18, 47, 56, 57, 58], target = 1\n",
      "context = 6, input = [18, 47, 56, 57, 58, 1], target = 15\n",
      "context = 7, input = [18, 47, 56, 57, 58, 1, 15], target = 47\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:55:34.766801Z",
     "start_time": "2024-12-12T04:55:34.736235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "\n",
    "def get_batch(data):\n",
    "    start_idx = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "\n",
    "    return torch.stack([data[i: i + context_length] for i in start_idx]), torch.stack(\n",
    "        [data[i + 1: i + 1 + context_length] for i in start_idx])\n",
    "\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print(\"input\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"target\")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ],
   "id": "1cfbc1a9310f0e2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "target\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:07:00.898684Z",
     "start_time": "2024-12-12T05:07:00.863524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLangModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        logits = self.token_embedding(indices)  # B = batch_size,T = context_length, C= vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, init_token, max_new_tokens):\n",
    "        sequence = init_token\n",
    "        for itr in range(max_new_tokens):\n",
    "            logits, loss = self(sequence)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # next_token = torch.argmax(probs, dim=-1)\n",
    "            # next_token = next_token.unsqueeze(1)\n",
    "            sequence = torch.cat((sequence, next_token), dim=1)\n",
    "        return sequence\n",
    "\n",
    "\n",
    "m = BigramLangModel(char_dataset1.get_vocab_size())\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "initial_token = torch.tensor(char_dataset1.encode('\\n'), dtype=torch.long).unsqueeze(0)\n",
    "# 0 == new line char\n",
    "print(\n",
    "    f\"Generated Sequence : {char_dataset1.decode(m.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "f8a41f136360200c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "Generated Sequence : \n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:00:44.637674Z",
     "start_time": "2024-12-12T05:00:43.362444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "xb2, yb2 = get_batch(train_data2)\n",
    "m2 = BigramLangModel(char_dataset2.get_vocab_size())\n",
    "logits, loss = m2(xb2, yb2)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "initial_token = torch.tensor(char_dataset2.encode('I Love You'), dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "print(\n",
    "    f\"Generated Sequence : {char_dataset2.decode(m2.generate(indices=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "c6758d9f31d26673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 10770])\n",
      "tensor(9.9281, grad_fn=<NllLossBackward0>)\n",
      "Generated Sequence : I Love You contewould RevoltALO First ample cry tideitted courage jade Whestuff Coyingbeseem Giv Despis exclaimscheshysicpipe dashovato spectatorssati interruptrgetfive kinsmenband unwa provinc growpierce arise Dick Suppl fare fatThat praisestcheryank inducedzard swim Pluck nowertake attireabsolve lead doth scornmi Monday orphan trudge lies fea qua Ea gulf dreadfulbaby lad Love Beggarsting knock chafeitedentiies kindred sitting parleGood valour oceanstateRAN scourabble cloudedper wedlock unre agreed emptiebo thereof does unfe perfume Hast cormorantMore uncl\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:11:41.287321Z",
     "start_time": "2024-12-12T05:11:37.522063Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)",
   "id": "37256f6942f4d361",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:27:54.315923Z",
     "start_time": "2024-12-12T05:27:54.310656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size == 32\n",
    "\n",
    "\n",
    "def train(model, data, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for _ in range(100):\n",
    "            xb, yb = get_batch(data)\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch}, Loss {total_loss / 100}\")\n"
   ],
   "id": "321f466874bcd788",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:27:55.801729Z",
     "start_time": "2024-12-12T05:27:55.040597Z"
    }
   },
   "cell_type": "code",
   "source": "train(m, train_data)",
   "id": "b0a9ce73e32b3974",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 2.481527715921402\n",
      "Epoch 1, Loss 2.4788355803489686\n",
      "Epoch 2, Loss 2.451487354040146\n",
      "Epoch 3, Loss 2.4628010988235474\n",
      "Epoch 4, Loss 2.461507182121277\n",
      "Epoch 5, Loss 2.486212739944458\n",
      "Epoch 6, Loss 2.4893881380558014\n",
      "Epoch 7, Loss 2.4833970165252683\n",
      "Epoch 8, Loss 2.4845554852485656\n",
      "Epoch 9, Loss 2.430941741466522\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:27:57.110033Z",
     "start_time": "2024-12-12T05:27:57.106996Z"
    }
   },
   "cell_type": "code",
   "source": "initial_token = torch.tensor(char_dataset1.encode('\\n'), dtype=torch.long).unsqueeze(0)",
   "id": "8a52bd6d2f8cb33",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:27:58.513201Z",
     "start_time": "2024-12-12T05:27:58.487920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Generated Sequence : {char_dataset1.decode(m.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "ac4d818700d43790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence : \n",
      "ORY CILLURind cenoy d DUSO:\n",
      "HUK:\n",
      "ARI's, I'sh s.\n",
      "\n",
      "Yow mutithak r VOMyo,\n",
      "The ivendonchifitolepeged ale\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:31:19.563715Z",
     "start_time": "2024-12-12T05:28:01.958287Z"
    }
   },
   "cell_type": "code",
   "source": "train(m2, train_data2)",
   "id": "ba9ee895c16db240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 9.804229316711426\n",
      "Epoch 1, Loss 9.798818016052246\n",
      "Epoch 2, Loss 9.782116651535034\n",
      "Epoch 3, Loss 9.812646226882935\n",
      "Epoch 4, Loss 9.800622415542602\n",
      "Epoch 5, Loss 9.815299606323242\n",
      "Epoch 6, Loss 9.811995611190795\n",
      "Epoch 7, Loss 9.824566326141358\n",
      "Epoch 8, Loss 9.803545007705688\n",
      "Epoch 9, Loss 9.802391748428345\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:33:24.927925Z",
     "start_time": "2024-12-12T05:33:24.903544Z"
    }
   },
   "cell_type": "code",
   "source": "initial_token = torch.tensor(char_dataset2.encode('I love '), dtype=torch.long).unsqueeze(0)",
   "id": "b878da9aaf23fc3",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T05:33:25.603771Z",
     "start_time": "2024-12-12T05:33:25.263698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Generated Sequence : {char_dataset2.decode(m2.generate(indices=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "d2ba3372c3d85ad6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence : I love irre letters warrant posterity Rich assure valley gripe condemn glean speeches trouble herdsm maidenhead envywarm swallow Juno gust supposes Both Wor lave held noeameaning wenches memory mockSA unluckingham ranillshol bird hood amitygold Sha bruise perpetu Seduc breathedNovalued handledoveramber shoulderorrectionanointed Conceive braverjury penitent like unshape froward sovereignty inkLOU Musician rode slippery faithfulexamine browsove climatelvish fixMILL sake softly pike confiscatPray quarr Hap reign anchor streak Wruccessive above likelihooddnes drops medlar past ALamesuring fashions closeturgeon Give friend\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed90cdcd270ad102"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
