{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:21:59.602246Z",
     "start_time": "2024-12-13T05:21:59.573833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "import sentencepiece as spm\n",
    "import subprocess\n",
    "import wandb\n",
    "import nltk\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "f9a7e8aa1b3814ff",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:21:59.685419Z",
     "start_time": "2024-12-13T05:21:59.676801Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip freeze > requirements.txt",
   "id": "e67ac312e97d42f",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:33:16.966107Z",
     "start_time": "2024-12-13T07:33:15.994008Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.login(key=\"2b242cad61896bc77d8053286a9c3e79f01c9127\")",
   "id": "bca6edd824b1b033",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/aria/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:00.328312Z",
     "start_time": "2024-12-13T05:22:00.321575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_requirements() -> bool:\n",
    "    try:\n",
    "        if not os.path.exists(\"requirements.txt\"):\n",
    "            raise FileNotFoundError(\"requirements.txt not found\")\n",
    "\n",
    "        result = subprocess.run(\n",
    "            [\"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "            check=True,  # Raise an exception if the command fails\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing requirements: {e.stderr}\")\n",
    "        return False"
   ],
   "id": "359f1802d1cc1390",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:00.332991Z",
     "start_time": "2024-12-13T05:22:00.330927Z"
    }
   },
   "cell_type": "code",
   "source": "# check_requirements()",
   "id": "37422fc4f98dd644",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:00.339751Z",
     "start_time": "2024-12-13T05:22:00.335020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embed = 32\n",
    "# ------------"
   ],
   "id": "5ec024fa984c31af",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:00.344549Z",
     "start_time": "2024-12-13T05:22:00.341673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(\"Dataset.txt\"):\n",
    "    os.system(\"wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "    os.rename(\"input.txt\", 'Dataset.txt')"
   ],
   "id": "fe02f43f8fac14da",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:00.361678Z",
     "start_time": "2024-12-13T05:22:00.346264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: str, mode: str = \"normal\"):\n",
    "\n",
    "        self.tokens = set(nltk.word_tokenize(data))\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"normal\":\n",
    "            self.chars = sorted(set(train_text))  # get characters from the input data\n",
    "\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}  # map characters to integer indices\n",
    "            self.itos = {i: ch for i, ch in enumerate(self.chars)}  # map integer indices to characters\n",
    "            self.vocab_size = len(self.chars)\n",
    "\n",
    "        elif mode == \"sentencepiece\":\n",
    "            self.vocab_size = min(len(self.tokens), 10770)\n",
    "            spm.SentencePieceTrainer.train(model_prefix='shakespeare', input='Dataset.txt',\n",
    "                                           vocab_size=10770, unk_id=0, bos_id=1, eos_id=2, pad_id=3)\n",
    "\n",
    "\n",
    "        elif mode == \"tiktoken\":\n",
    "            self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "            self.vocab_size = self.enc.max_token_value + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.mode == \"normal\":\n",
    "            return [self.stoi[s] for s in text]\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.encode(text)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if self.mode == \"normal\":\n",
    "            return ''.join([self.itos[t] for t in tokens])\n",
    "        elif self.mode == \"sentencepiece\":\n",
    "            sp = spm.SentencePieceProcessor(model_file='shakespeare.model')\n",
    "            return sp.decode(tokens)\n",
    "        elif self.mode == \"tiktoken\":\n",
    "            return self.enc.decode(tokens)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "\n",
    "    # ```"
   ],
   "id": "7bba9bcbc5272509",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:00.369084Z",
     "start_time": "2024-12-13T05:22:00.363707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"Dataset.txt\", \"r\") as file:\n",
    "    train_text = file.read()\n",
    "\n",
    "print(train_text[:500])"
   ],
   "id": "aa83771b2f7c2d6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:08.104230Z",
     "start_time": "2024-12-13T05:22:00.372782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_encoding = CharDataset(train_text, mode=\"normal\")\n",
    "sent_piece = CharDataset(train_text, mode=\"sentencepiece\")\n",
    "tiktoken_encoding = CharDataset(train_text, mode=\"tiktoken\")"
   ],
   "id": "3c69851ade7e8691",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:08.709424Z",
     "start_time": "2024-12-13T05:22:08.111787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Normal encoding: Length of sequence = {len(normal_encoding.encode(train_text))}, Vocab size = {normal_encoding.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"SentencePiece encoding: Length of sequence = {len(sent_piece.encode(train_text))}, Vocab size = {sent_piece.get_vocab_size()}\")\n",
    "\n",
    "print(\n",
    "    f\"TikToken encoding: Length of sequence = {len(tiktoken_encoding.encode(train_text))}, Vocab size = {tiktoken_encoding.get_vocab_size()}\")"
   ],
   "id": "8dca0b7739b1d395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal encoding: Length of sequence = 1115394, Vocab size = 65\n",
      "SentencePiece encoding: Length of sequence = 290364, Vocab size = 10770\n",
      "TikToken encoding: Length of sequence = 338025, Vocab size = 50257\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:08.932054Z",
     "start_time": "2024-12-13T05:22:08.711599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor(normal_encoding.encode(train_text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ],
   "id": "a5ca390e5cf5a7fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.215767Z",
     "start_time": "2024-12-13T05:22:08.933328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data2 = torch.tensor(sent_piece.encode(train_text), dtype=torch.long)\n",
    "print(data2.shape, data2.dtype)\n",
    "print(data2[:1000])"
   ],
   "id": "378307723b7d6e7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([290364]) torch.int64\n",
      "tensor([  160,   346,     5,  1001,    54,  1671,   208,   953,     4,   181,\n",
      "           27,   147,     6,   421,     5,   997,     4,   147,     6,   160,\n",
      "          346,     5,   112,    58,    47,  1968,   540,    10,   292,   117,\n",
      "           10,  4422,    19,   421,     5,  7385,     6,  1968,     6,   160,\n",
      "          346,     5,   160,     4,    15,   109,  1602,   479,    26,  1795,\n",
      "          785,    10,     7,   397,     6,   421,     5,   184,   109,     8,\n",
      "           72,     4,    54,   109,     8,    72,     6,   160,   346,     5,\n",
      "          248,    96,   461,    37,     4,    11,    54,     8,    65,    34,\n",
      "         1763,    78,    59,   227,  3029,     6,   244,     8,    72,    16,\n",
      "         7476,    19,   421,     5,   165,    73,  4082,    64,     8,    72,\n",
      "           13,   107,    29,    28,   230,     5,   293,     4,   293,    21,\n",
      "           92,   282,   346,     5,   727,   314,     4,    68,  1339,     6,\n",
      "          160,   346,     5,   184,    58,  7523,   289,  1339,     4,     7,\n",
      "         2571,    68,     6,    69,  1818,  7691,    64,    84,  4491,    96,\n",
      "            5,    74,    89,    84,   855,    96,    44,     7,  6933,     4,\n",
      "          626,    29,   114,  2357,     4,    54,   384,  1376,    89,  4491,\n",
      "           24,    96,  5998,   111,    13,    44,    89,   187,    54,    58,\n",
      "          143,   419,     5,     7,  5515,    22,  2970,    12,    96,     4,\n",
      "            7,  2806,    14,    59,  2033,     4,    26,    40,   126, 10239,\n",
      "         2700,    10,  2025,  8022,    86,  5213,    13,    59,  3862,    26,\n",
      "           16,  1784,    10,    83,   248,    96,   776,    36,    30,    59,\n",
      "         5599,     4,   467,    54,   903,    18,  7729,     5,    32,     7,\n",
      "          599,   109,     9,   147,    36,    20,  4617,    32,  3111,     4,\n",
      "           25,    20,  4328,    32,   776,     6,    92,   282,   346,     5,\n",
      "          523,    15,  1671,  1232,  4262,   334,  1602,   479,    19,   421,\n",
      "            5,    66,   689,    37,   365,     5,    35,     8,    12,    16,\n",
      "          238,  1261,    10,     7,   714,  7982,     6,    92,   282,   346,\n",
      "            5,   120,  4766,    15,    63,  2123,    35,   530,   230,    32,\n",
      "           33,   620,    19,   160,   346,     5,  2315,   141,   101,    13,\n",
      "           11,   388,    28,   743,    10,   162,    37,    68,   734,    32,\n",
      "           72,     4,    44,    22,    35,  1087,    12,   353,    30,   262,\n",
      "          580,     6,    92,   282,   346,     5,   323,     4,    44,   147,\n",
      "           25,  6050,   111,     6,   160,   346,     5,     9,    95,   361,\n",
      "           15,     4,    63,    35,   106,   230,  7337,     4,    35,   137,\n",
      "           29,    10,    22,   524,     5,   372,  1093,    39,  8907,    24,\n",
      "          204,   167,    28,   743,    10,    95,    29,    81,    32,    33,\n",
      "          620,    35,   137,    29,    10,   490,    33,   283,    11,    10,\n",
      "           28,   363,   111,   580,    13,   129,    35,    26,     4,   529,\n",
      "          371,     7,    16,  9775,    14,    33,   945,     6,    92,   282,\n",
      "          346,     5,    69,    35,   214,   458,    20,    33,   588,     4,\n",
      "           15,  1527,    16,  1519,    20,    37,     6,   112,   116,    20,\n",
      "           61,   251,    95,    35,    26,  5537,   491,     6,   160,   346,\n",
      "            5,   132,     9,   116,    25,     4,     9,   442,    25,    28,\n",
      "         2562,    14,  5872,    13,    35,   106,  1126,     4,    30,  6751,\n",
      "            4,    10,  4031,    20,  4386,     6,    69,  5748,    58,   163,\n",
      "           19,    45,   270,   861,   201,     8,     7,   617,    26,  1398,\n",
      "          153,     5,   415,   310,    54,  3820,    91,    19,    10,     7,\n",
      "         1683,    21,   421,     5,   177,     4,    98,     6,   160,   346,\n",
      "            5,  3142,    21,   236,   308,    91,    19,    92,   282,   346,\n",
      "            5,  2265,   141,  1684,  8775,  7840,   644,    13,   103,    22,\n",
      "          106,  1625,  1100,     7,   397,     6,   160,   346,     5,   127,\n",
      "            8,    12,   103,   864,   576,     5,    84,    47,     7,   355,\n",
      "          114,    41,    21,    18,   303,   267,   100,     5,    69,   832,\n",
      "            8,    12,     4,    17,  2796,     4,    20,   217,    19,   215,\n",
      "          131,    15,   152,  5745,    11,  7326,    19,    45,   550,    19,\n",
      "          147,     4,     9,   307,    15,     6,   160,   346,     5,   456,\n",
      "          546,    26,    25,  2193,    10,     7,  1608,    13,    89,    34,\n",
      "          128,    20,  9781,    36,    32,    72,   890,    63,    54,  1876,\n",
      "           10,    57,     4,   129,    75,    54,     8,    65,   431,    52,\n",
      "         1057,    20,  1082,     6,   430,    95,   289,  3152,    34,   975,\n",
      "         6245,     5,    89,    55,   109,    54,    34,   975,   568,   143,\n",
      "            6,    18,   303,   267,   100,     5,   156,     4,  1489,     4,\n",
      "           17,    68,   305,     4,   154,   864,  3563,     4,   396,    15,\n",
      "         3589,  1333,    19,   160,   346,     5,   184,   214,     4,    79,\n",
      "            4,    54,    58,  1811,   979,     6,    18,   303,   267,   100,\n",
      "            5,     9,   180,    15,     4,   305,     4,   188,  6376,   946,\n",
      "          692,   370,     7,  2571,    14,    15,     6,    77,    31,  2273,\n",
      "            4,   207,  4019,    20,    36,  4056,     4,    15,   110,    40,\n",
      "          101,  2983,    78,     7,   274,    30,    31,  8351,  6336,    40,\n",
      "         2841,    83,    66,   689,     7,  1084,   457,     4,   440,   862,\n",
      "           42,    64,    45,   251,    29,  3592,     4,  1877,    51,   896,\n",
      "          484,  5711,   175,    73,   975,  4546,  7299,   117,   167,   298,\n",
      "         3360,    20,    31,  4371,     6,    77,     7,  4056,     4,    45,\n",
      "          599,     4,    25,     7,  2571,     4,   108,    29,     4,    11,\n",
      "          207,  1345,    10,    83,     4,    25,   568,     4,   116,   458,\n",
      "            6,  1481,     4,   112,    58,  4562,    53,  3756,  1032,  1310,\n",
      "          215,    73,   965,    12,    15,     4,    11,    15,  1166,    45,\n",
      "         5852,   201,     8,     7,   457,     4,   236,   692,    32,    15,\n",
      "          122,  2876,     4,   234,    15,   850,    83,    40,   900,     6,\n",
      "          160,   346,     5,   120,  3191,    32,    96,    21,  1557,     4,\n",
      "          516,    21,   430,   635,     8,    80,   692,    24,    32,    96,\n",
      "          142,     5,  1104,    96,    10,  4422,     4,    11,    86,  2160,\n",
      "           39, 10413,  3207,  8923,    30,  2710,    13,   108,  5297,    12,\n",
      "           32,    96,  8069,     4,    10,  3523,  4601,    12,    13,  2535,\n",
      "         2353,   208,  2357,  1267,  5239,    88,   334,     7,   895,     4,\n",
      "           11,  2957,    73,  3265,    51,  3915,  2353,     4,    10,  5141,\n",
      "          144,    11,  3408,     7,   289,     6,   132,     7,  1069,  1440,\n",
      "           96,    25,   144,     4,    89,    42,    13,    11,   125,     8,\n",
      "           12,    47,     7,    93,    89,   294,    96,     6,    18,   303,\n",
      "          267,   100,     5,   347,  1310,    15,   116,  3747,  1333,  2744,\n",
      "         6050,     4,   212,    28,  2419,    14,  2041,     6,     9,    55,\n",
      "          180,    15,    66,  1140,   858,     5,    29,   110,    28,    15,\n",
      "           34,   455,    29,    13,    67,     4,   447,    29,  2278,    17,\n",
      "          696,     4,     9,    42,  3876,    46,  3050,    52,    72,    16,\n",
      "          336,    73,     6,   160,   346,     5,   319,     4,     9,     8,\n",
      "           65,   181,    29,     4,    79,     5,   142,    15,   116,    25,\n",
      "          187,    10,  2111,  2257,   332,    59,  2507,    30,    16,   858,\n",
      "            5,    44,     4,   126,    52,    72,   490,    15,     4,   881,\n",
      "            6,    18,   303,   267,   100,     5,   342,    81,    16,   155,\n",
      "          149,    47,     7,   508,     8,    12,  3443,  7126,     8,    24])\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.222371Z",
     "start_time": "2024-12-13T05:22:09.217027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "b42876778837d857",
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.228586Z",
     "start_time": "2024-12-13T05:22:09.224763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = 8\n",
    "\n",
    "print(train_data[:context_length])\n",
    "print(normal_encoding.decode(train_data[:context_length].tolist()))"
   ],
   "id": "f9a364fcef57da41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "First Ci\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.251336Z",
     "start_time": "2024-12-13T05:22:09.230877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data2 = data2[:n]\n",
    "val_data2 = data2[n:]\n",
    "\n",
    "print(train_data2[:context_length])\n",
    "print(sent_piece.decode(train_data2[:context_length].tolist()))"
   ],
   "id": "82823b888661a6e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 160,  346,    5, 1001,   54, 1671,  208,  953])\n",
      "First Citizen: Before we proceed any further\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.259532Z",
     "start_time": "2024-12-13T05:22:09.252639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length + 1]\n",
    "\n",
    "for context in range(1, context_length):\n",
    "    print(f\"context = {context}, input = {x[:context].tolist()}, target = {y[context - 1]}\")"
   ],
   "id": "5aa32008cc25e3a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context = 1, input = [18], target = 47\n",
      "context = 2, input = [18, 47], target = 56\n",
      "context = 3, input = [18, 47, 56], target = 57\n",
      "context = 4, input = [18, 47, 56, 57], target = 58\n",
      "context = 5, input = [18, 47, 56, 57, 58], target = 1\n",
      "context = 6, input = [18, 47, 56, 57, 58, 1], target = 15\n",
      "context = 7, input = [18, 47, 56, 57, 58, 1, 15], target = 47\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.301840Z",
     "start_time": "2024-12-13T05:22:09.260748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "\n",
    "def get_batch(data):\n",
    "    start_idx = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[i: i + context_length] for i in start_idx])\n",
    "    y = torch.stack([data[i + 1: i + 1 + context_length] for i in start_idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print(\"input\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"target\")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ],
   "id": "1cfbc1a9310f0e2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "target\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.311941Z",
     "start_time": "2024-12-13T05:22:09.303767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLangModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=32):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position = nn.Embedding(context_length, n_embed)\n",
    "        self.langhead = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        # T: sequence length (number of tokens) , B: batch size (number of sequences)\n",
    "        B, T = indices.shape\n",
    "        tok_embeds = self.token_embedding(indices)  # (B, T, n_embed)\n",
    "        pos_embeds = self.position(torch.arange(T, device=indices.device))  # (T, n_embed)\n",
    "        x = tok_embeds + pos_embeds  # (B, T, n_embed)\n",
    "        logits = self.langhead(self.token_embedding(indices))  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, init_token, max_new_tokens):\n",
    "        sequence = init_token\n",
    "        for itr in range(max_new_tokens):\n",
    "            logits, loss = self(sequence)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # next_token = torch.argmax(probs, dim=-1)\n",
    "            # next_token = next_token.unsqueeze(1)\n",
    "            sequence = torch.cat((sequence, next_token), dim=1)\n",
    "        return sequence"
   ],
   "id": "f8a41f136360200c",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.374421Z",
     "start_time": "2024-12-13T05:22:09.315928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BigramLangModel(normal_encoding.get_vocab_size(), n_embed)\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "initial_token = torch.tensor(normal_encoding.encode('\\n'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "# 0 == new line char\n",
    "print(\n",
    "    f\"Generated Sequence : {normal_encoding.decode(m.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "714c9e89761a1913",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "tensor(4.4922, grad_fn=<NllLossBackward0>)\n",
      "Generated Sequence : \n",
      "lN!BJ'kysLCMFJPKOL?DP-QWwrEoL?jLDJQOL.f'RIHD'Hdhs Yv,wxatnscMZwtEOS'palkq3ssZeAvzF-QT;eMk;x.gQSFCLgx\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.735069Z",
     "start_time": "2024-12-13T05:22:09.376896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "xb2, yb2 = get_batch(train_data2)\n",
    "m2 = BigramLangModel(sent_piece.get_vocab_size(), n_embed)\n",
    "m2 = m2.to(device)\n",
    "logits, loss = m2(xb2, yb2)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "initial_token = torch.tensor(sent_piece.encode('I Love You'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "print(\n",
    "    f\"Generated Sequence : {sent_piece.decode(m2.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())}\")"
   ],
   "id": "c6758d9f31d26673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 10770])\n",
      "tensor(9.4203, grad_fn=<NllLossBackward0>)\n",
      "Generated Sequence : I Love You magn cruel judgecarce squawelve burial livery honey Herequal searshedthreerman publi apparent vast fester furnished Katect Env sends quaimes spot strange metal stirring unfeiding impedimentwould manaclerefulbishopconddeserving iticed distinguish create obscured suit difference flour meekBA bettercannon visitor goddessdestABHold valour embassies moveouls darings steeds Olymp region intobuil hatefulpeaceRK glori necessaries roaderial legundation chair whose nail four detest talkingGating shot Ingrat louder butterflyam sunderozenWhy Lassel mann accompanied perdition hadst safety soever\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.746938Z",
     "start_time": "2024-12-13T05:22:09.736636Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer1 = torch.optim.Adam(m.parameters(), lr=learning_rate)",
   "id": "37256f6942f4d361",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.754967Z",
     "start_time": "2024-12-13T05:22:09.748852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(train_data, val_data):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for data in [train_data, val_data]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out['train' if data is train_data else 'val'] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "6fd4d1ae6090ee90",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:22:09.765687Z",
     "start_time": "2024-12-13T05:22:09.758002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size == 32\n",
    "\n",
    "\n",
    "def train(model, data, optimizer, max_iters=3000, epochs=10, steps=100, eval_interval=300, wandb_log=True):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for iter in range(max_iters):\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if iter % eval_interval == 0:\n",
    "                losses = estimate_loss(train_data, val_data)\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                total_loss += losses['train']\n",
    "\n",
    "                wandb.log({\"Iteration\": iter, \"Average Loss\": losses['train'], \"Val Loss\": losses['val']})\n",
    "\n",
    "            xb, yb = get_batch(data)\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"epoch {epoch}: avg loss: {total_loss * eval_interval / max_iters}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\"Epoch\": epoch, \"Total Loss\": total_loss * eval_interval / max_iters})\n"
   ],
   "id": "321f466874bcd788",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:23:04.992198Z",
     "start_time": "2024-12-13T05:22:09.767730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.init(\n",
    "    project=\"LLM\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"Simple BigramLangModel\",\n",
    "        \"dataset\": \"Shakespeare\",\n",
    "    },\n",
    "    name=\"Normal Encoding\"\n",
    ")\n",
    "\n",
    "train(m, train_data, optimizer1)"
   ],
   "id": "6ae2562366df56c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aria/opt/anaconda3/lib/python3.8/site-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/aria/Desktop/unige/Deep Learning/Project/LLM/wandb/run-20241213_062209-qakymab8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/qakymab8' target=\"_blank\">Normal Encoding</a></strong> to <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/qakymab8' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/qakymab8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3769, val loss 4.3673\n",
      "step 300: train loss 2.6203, val loss 2.6888\n",
      "step 600: train loss 2.6019, val loss 2.6276\n",
      "step 900: train loss 2.5759, val loss 2.5812\n",
      "step 1200: train loss 2.5494, val loss 2.5526\n",
      "step 1500: train loss 2.5701, val loss 2.6050\n",
      "step 1800: train loss 2.5565, val loss 2.5492\n",
      "step 2100: train loss 2.5932, val loss 2.6070\n",
      "step 2400: train loss 2.5580, val loss 2.5640\n",
      "step 2700: train loss 2.5348, val loss 2.5481\n",
      "epoch 0: avg loss: 2.753690004348755\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5442, val loss 2.5824\n",
      "step 300: train loss 2.5174, val loss 2.5132\n",
      "step 600: train loss 2.5208, val loss 2.5361\n",
      "step 900: train loss 2.5105, val loss 2.5478\n",
      "step 1200: train loss 2.5107, val loss 2.5505\n",
      "step 1500: train loss 2.5220, val loss 2.5511\n",
      "step 1800: train loss 2.5733, val loss 2.5594\n",
      "step 2100: train loss 2.5349, val loss 2.5415\n",
      "step 2400: train loss 2.5280, val loss 2.5352\n",
      "step 2700: train loss 2.5363, val loss 2.5723\n",
      "epoch 1: avg loss: 2.5298142433166504\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5245, val loss 2.5501\n",
      "step 300: train loss 2.5343, val loss 2.5716\n",
      "step 600: train loss 2.5272, val loss 2.5593\n",
      "step 900: train loss 2.5135, val loss 2.5344\n",
      "step 1200: train loss 2.5594, val loss 2.5547\n",
      "step 1500: train loss 2.5283, val loss 2.5325\n",
      "step 1800: train loss 2.5336, val loss 2.5613\n",
      "step 2100: train loss 2.5160, val loss 2.5427\n",
      "step 2400: train loss 2.4981, val loss 2.5358\n",
      "step 2700: train loss 2.5650, val loss 2.5676\n",
      "epoch 2: avg loss: 2.5299935340881348\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5354, val loss 2.5796\n",
      "step 300: train loss 2.5115, val loss 2.5654\n",
      "step 600: train loss 2.5298, val loss 2.5639\n",
      "step 900: train loss 2.5179, val loss 2.5650\n",
      "step 1200: train loss 2.5228, val loss 2.5706\n",
      "step 1500: train loss 2.5103, val loss 2.5449\n",
      "step 1800: train loss 2.5284, val loss 2.5484\n",
      "step 2100: train loss 2.4993, val loss 2.5550\n",
      "step 2400: train loss 2.5024, val loss 2.5467\n",
      "step 2700: train loss 2.5464, val loss 2.5432\n",
      "epoch 3: avg loss: 2.5204219818115234\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5094, val loss 2.5534\n",
      "step 300: train loss 2.4808, val loss 2.5288\n",
      "step 600: train loss 2.5398, val loss 2.5299\n",
      "step 900: train loss 2.5344, val loss 2.5631\n",
      "step 1200: train loss 2.5144, val loss 2.5031\n",
      "step 1500: train loss 2.5251, val loss 2.5363\n",
      "step 1800: train loss 2.5441, val loss 2.5544\n",
      "step 2100: train loss 2.5250, val loss 2.5621\n",
      "step 2400: train loss 2.5088, val loss 2.5779\n",
      "step 2700: train loss 2.5440, val loss 2.5410\n",
      "epoch 4: avg loss: 2.522576332092285\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5526, val loss 2.5720\n",
      "step 300: train loss 2.4985, val loss 2.5698\n",
      "step 600: train loss 2.5261, val loss 2.5314\n",
      "step 900: train loss 2.5148, val loss 2.5605\n",
      "step 1200: train loss 2.4928, val loss 2.5274\n",
      "step 1500: train loss 2.5256, val loss 2.5249\n",
      "step 1800: train loss 2.5350, val loss 2.5029\n",
      "step 2100: train loss 2.5179, val loss 2.5351\n",
      "step 2400: train loss 2.5196, val loss 2.5643\n",
      "step 2700: train loss 2.5251, val loss 2.5458\n",
      "epoch 5: avg loss: 2.5208046436309814\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5066, val loss 2.5291\n",
      "step 300: train loss 2.5187, val loss 2.5594\n",
      "step 600: train loss 2.5306, val loss 2.5175\n",
      "step 900: train loss 2.5173, val loss 2.5363\n",
      "step 1200: train loss 2.4999, val loss 2.5283\n",
      "step 1500: train loss 2.5091, val loss 2.5299\n",
      "step 1800: train loss 2.4846, val loss 2.5462\n",
      "step 2100: train loss 2.5213, val loss 2.5579\n",
      "step 2400: train loss 2.5219, val loss 2.5360\n",
      "step 2700: train loss 2.5004, val loss 2.5535\n",
      "epoch 6: avg loss: 2.5110199451446533\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5352, val loss 2.5589\n",
      "step 300: train loss 2.4910, val loss 2.5614\n",
      "step 600: train loss 2.4972, val loss 2.4975\n",
      "step 900: train loss 2.4864, val loss 2.5270\n",
      "step 1200: train loss 2.5078, val loss 2.5336\n",
      "step 1500: train loss 2.5450, val loss 2.5221\n",
      "step 1800: train loss 2.5220, val loss 2.5558\n",
      "step 2100: train loss 2.5262, val loss 2.5189\n",
      "step 2400: train loss 2.5032, val loss 2.5197\n",
      "step 2700: train loss 2.5283, val loss 2.5185\n",
      "epoch 7: avg loss: 2.5142288208007812\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.5347, val loss 2.5355\n",
      "step 300: train loss 2.5326, val loss 2.5303\n",
      "step 600: train loss 2.4990, val loss 2.5544\n",
      "step 900: train loss 2.5004, val loss 2.5484\n",
      "step 1200: train loss 2.5337, val loss 2.5509\n",
      "step 1500: train loss 2.5150, val loss 2.5138\n",
      "step 1800: train loss 2.5310, val loss 2.5612\n",
      "step 2100: train loss 2.5108, val loss 2.5471\n",
      "step 2400: train loss 2.5032, val loss 2.5098\n",
      "step 2700: train loss 2.5180, val loss 2.4950\n",
      "epoch 8: avg loss: 2.517847776412964\n",
      "--------------------------------------------------\n",
      "step 0: train loss 2.4886, val loss 2.5727\n",
      "step 300: train loss 2.5516, val loss 2.5478\n",
      "step 600: train loss 2.5369, val loss 2.5486\n",
      "step 900: train loss 2.5169, val loss 2.5203\n",
      "step 1200: train loss 2.5208, val loss 2.5153\n",
      "step 1500: train loss 2.4932, val loss 2.5428\n",
      "step 1800: train loss 2.5085, val loss 2.5193\n",
      "step 2100: train loss 2.5228, val loss 2.5551\n",
      "step 2400: train loss 2.5155, val loss 2.5349\n",
      "step 2700: train loss 2.5334, val loss 2.5351\n",
      "epoch 9: avg loss: 2.518813371658325\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:23:08.016242Z",
     "start_time": "2024-12-13T05:23:05.000927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_token = torch.tensor(normal_encoding.encode('\\n'), dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "generated_text = normal_encoding.decode(m.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())\n",
    "\n",
    "print(\n",
    "    f\"Generated Sequence : {generated_text}\")\n",
    "\n",
    "wandb.log({\"Generated Text\": generated_text})\n",
    "wandb.finish()"
   ],
   "id": "ac4d818700d43790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence : \n",
      "Mugotil my nt d sis? bed'd n aslng, m he ou thayo th my: ncet buse ETIOLEOLUKES:\n",
      "\n",
      "\n",
      "BUS prem CI willo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Loss</td><td>█▇▆▅▇▂▂▃▄▃▂▅▂▃▂▄▂▄▄▄▄▂▃▄▃▃▃▃▂▃▂▁▂▃▂▃▂▁▂▃</td></tr><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Iteration</td><td>▂▆█▁▃▅▆▇▃▆█▄▅▆▆▁▂▃▅█▂▅▆▆█▂▃▆█▂▃▄▆▆▁▆█▂▃▆</td></tr><tr><td>Total Loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Loss</td><td>2.53338</td></tr><tr><td>Epoch</td><td>9</td></tr><tr><td>Generated Text</td><td><br>Mugotil my nt d sis...</td></tr><tr><td>Iteration</td><td>2700</td></tr><tr><td>Total Loss</td><td>2.51881</td></tr><tr><td>Val Loss</td><td>2.53505</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Normal Encoding</strong> at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/qakymab8' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/qakymab8</a><br> View project at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241213_062209-qakymab8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:23:12.144581Z",
     "start_time": "2024-12-13T05:23:08.023519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.init(\n",
    "    project=\"LLM\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"Simple BigramLangModel\",\n",
    "        \"dataset\": \"Shakespeare\",\n",
    "    },\n",
    "    name=\"Sentencepiece Encoding\"\n",
    ")\n",
    "\n",
    "optimizer2 = torch.optim.Adam(m2.parameters(), lr=learning_rate)\n",
    "train(m2, train_data2, optimizer2, max_iters=100, epochs=1, steps=10, eval_interval=10)"
   ],
   "id": "ba9ee895c16db240",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aria/opt/anaconda3/lib/python3.8/site-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/aria/Desktop/unige/Deep Learning/Project/LLM/wandb/run-20241213_062308-5ckt3534</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/5ckt3534' target=\"_blank\">Sentencepiece Encoding</a></strong> to <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/5ckt3534' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/5ckt3534</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4849, val loss 2.5206\n",
      "step 10: train loss 2.5267, val loss 2.5294\n",
      "step 20: train loss 2.4965, val loss 2.5253\n",
      "step 30: train loss 2.4867, val loss 2.5429\n",
      "step 40: train loss 2.5397, val loss 2.5361\n",
      "step 50: train loss 2.5019, val loss 2.5382\n",
      "step 60: train loss 2.5200, val loss 2.5249\n",
      "step 70: train loss 2.5044, val loss 2.5426\n",
      "step 80: train loss 2.5184, val loss 2.5091\n",
      "step 90: train loss 2.5228, val loss 2.5312\n",
      "epoch 0: avg loss: 2.510204792022705\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:23:14.664820Z",
     "start_time": "2024-12-13T05:23:12.147502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_token = torch.tensor(sent_piece.encode('I love'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "generated_text = sent_piece.decode(m2.generate(init_token=initial_token, max_new_tokens=100)[0].tolist())\n",
    "wandb.log({\"Generated Sequence\": generated_text})\n",
    "\n",
    "print(f\"Generated Sequence : {generated_text}\")\n",
    "wandb.finish()"
   ],
   "id": "d2ba3372c3d85ad6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence : I love  utteredsRUedit officious give How Hence hatefulig lead ornaments Anointed suborn showing batt cleans abide bore Bohemia tribute usagesequencesteem By patroness as honour long herdsm strengthreserved ambling deed wai tricks Christendomtinctkly counterpoi returneddeck veri hoa nest obedient trudge justice stark scholar things fret taper TushRE godrevailallow ground her allegiance steward grim after By No buzzard Miranda seize characters forcAU stray maidenheadsmperlemish endow savage namedessenger bells wives ju kinsman Repair obtain long pestilence keys Lead nip breathing fully bustle blot puritencomb revive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Loss</td><td>▁▆▂▁█▃▅▃▅▆</td></tr><tr><td>Epoch</td><td>▁</td></tr><tr><td>Iteration</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Total Loss</td><td>▁</td></tr><tr><td>Val Loss</td><td>▃▅▄█▇▇▄█▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Loss</td><td>2.5228</td></tr><tr><td>Epoch</td><td>0</td></tr><tr><td>Generated Sequence</td><td>I love  utteredsRUed...</td></tr><tr><td>Iteration</td><td>90</td></tr><tr><td>Total Loss</td><td>2.5102</td></tr><tr><td>Val Loss</td><td>2.53118</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Sentencepiece Encoding</strong> at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/5ckt3534' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM/runs/5ckt3534</a><br> View project at: <a href='https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM' target=\"_blank\">https://wandb.ai/ariamosavefar-universit-de-gen-ve/LLM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241213_062308-5ckt3534/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:01:02.948723Z",
     "start_time": "2024-12-13T12:01:02.755969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single Head perform self-attention:\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "key = key(x)  # (B, T, head_size)\n",
    "query = query(x)  # (B, T, head_size)\n",
    "v = value(x)  # (B, T, head_size)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = query @ key.transpose(-2,\n",
    "                            -1) * head_size ** 0.5  # dimension -2:T,-1:head_size.  (B,T,head)@(B,T,head) = (B, T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "temprature = 1\n",
    "wei = F.softmax(wei / temprature, dim=-1)\n",
    "\n",
    "out = wei @ v\n",
    "# out = wei @ x\n",
    "\n",
    "out.shape"
   ],
   "id": "e730d1e07d0a473e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:01:04.330523Z",
     "start_time": "2024-12-13T12:01:04.317963Z"
    }
   },
   "cell_type": "code",
   "source": "wei",
   "id": "fa3ed7da611461a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.2155e-03, 9.9878e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.2126e-02, 4.6801e-03, 9.8319e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.8567e-01, 1.7389e-03, 1.1154e-02, 1.4340e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.9030e-06, 3.1255e-04, 1.2404e-05, 1.4743e-06, 9.9967e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.3232e-07, 2.3691e-02, 9.7002e-07, 2.8430e-08, 9.7631e-01,\n",
       "          6.2551e-11, 0.0000e+00, 0.0000e+00],\n",
       "         [2.7302e-02, 9.1365e-01, 1.2346e-04, 9.9611e-05, 4.0265e-03,\n",
       "          5.4756e-02, 3.9283e-05, 0.0000e+00],\n",
       "         [2.0096e-05, 5.2594e-03, 9.8801e-04, 2.8998e-01, 1.1226e-03,\n",
       "          2.6327e-03, 3.5913e-01, 3.4087e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.6930e-03, 9.9831e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.5373e-02, 2.8518e-05, 9.8460e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.5568e-01, 1.4522e-03, 3.4180e-01, 1.0736e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.1405e-09, 1.2034e-05, 2.2405e-06, 9.9884e-01, 1.1502e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.1893e-04, 3.9617e-04, 1.8184e-05, 9.9484e-01, 4.6286e-03,\n",
       "          1.4409e-08, 0.0000e+00, 0.0000e+00],\n",
       "         [3.1441e-04, 3.5078e-01, 8.2858e-07, 2.0491e-01, 2.3099e-01,\n",
       "          1.8490e-01, 2.8101e-02, 0.0000e+00],\n",
       "         [6.6015e-01, 1.8817e-04, 1.2846e-03, 2.7599e-05, 3.3657e-01,\n",
       "          1.4030e-03, 1.5938e-04, 2.2000e-04]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.2843e-01, 5.7157e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.3328e-02, 6.7632e-01, 3.1035e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.4634e-09, 9.9474e-01, 1.6724e-05, 5.2393e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.9996e-01, 6.1124e-06, 1.5579e-05, 2.3253e-05, 4.1346e-08,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.2304e-02, 1.9717e-01, 1.1763e-02, 1.9313e-02, 1.1812e-01,\n",
       "          6.3132e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [8.6332e-08, 5.9802e-04, 9.5560e-05, 4.3797e-01, 1.5319e-02,\n",
       "          5.2808e-01, 1.7932e-02, 0.0000e+00],\n",
       "         [7.3300e-02, 9.7019e-02, 3.9485e-04, 1.3281e-01, 3.8682e-02,\n",
       "          4.5127e-02, 1.4929e-05, 6.1265e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.0447e-01, 9.5533e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.4891e-02, 9.8511e-01, 7.1828e-10, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.7280e-01, 2.2322e-01, 3.7295e-03, 2.5317e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.3923e-02, 9.4788e-01, 3.0543e-05, 2.3360e-02, 4.8018e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.5041e-04, 9.9967e-01, 2.6413e-11, 9.3374e-05, 8.2166e-05,\n",
       "          8.6310e-07, 0.0000e+00, 0.0000e+00],\n",
       "         [6.8663e-05, 6.5966e-05, 7.8839e-04, 1.8276e-05, 1.0165e-03,\n",
       "          9.9751e-01, 5.3552e-04, 0.0000e+00],\n",
       "         [9.0377e-05, 5.6961e-01, 1.2662e-04, 4.0560e-02, 4.1150e-02,\n",
       "          1.1812e-03, 4.7596e-03, 3.4252e-01]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:04:38.753748Z",
     "start_time": "2024-12-13T09:04:38.726049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size ** -0.5"
   ],
   "id": "362cc505d24f44f5",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:04:39.434987Z",
     "start_time": "2024-12-13T09:04:39.402001Z"
    }
   },
   "cell_type": "code",
   "source": "k.var()",
   "id": "dbb10ef28d10cb8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:04:40.138945Z",
     "start_time": "2024-12-13T09:04:40.131933Z"
    }
   },
   "cell_type": "code",
   "source": "q.var()",
   "id": "c880ba4fdeba305a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:04:40.708520Z",
     "start_time": "2024-12-13T09:04:40.703170Z"
    }
   },
   "cell_type": "code",
   "source": "wei.var()",
   "id": "3678ced0a7ece0fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:04:41.406848Z",
     "start_time": "2024-12-13T09:04:41.386050Z"
    }
   },
   "cell_type": "code",
   "source": "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)",
   "id": "2e40e7bbcdc39591",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:04:42.204139Z",
     "start_time": "2024-12-13T09:04:42.196673Z"
    }
   },
   "cell_type": "code",
   "source": "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)  # gets too peaky, converges to one-hot",
   "id": "eb9ce9af0038d426",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:05:05.529218Z",
     "start_time": "2024-12-13T09:05:05.509439Z"
    }
   },
   "cell_type": "code",
   "source": "x[:, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs",
   "id": "871553560e0df463",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1431), tensor(1.0705))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:05:06.262363Z",
     "start_time": "2024-12-13T09:05:06.252574Z"
    }
   },
   "cell_type": "code",
   "source": "x[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features",
   "id": "8135f5c3d6393c19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0073), tensor(1.0177))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# _______________________________________",
   "id": "cc51c829c2ee7c01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "275267758ba7baed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 14,
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "c995c856f0305fca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 15,
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei / Tempreture, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ],
   "id": "ad93f3d25ee36141"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 16,
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ],
   "id": "f0360f9d483911ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 17,
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "42253282f15f5b59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 18,
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ],
   "id": "b2c6c7cc062e94dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ],
   "id": "321b28d6ca3bf2b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44,
   "source": "wandb.login(key=\"2b242cad61896bc77d8053286a9c3e79f01c9127\")",
   "id": "417f3d61006898b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "execution_count": 45,
   "source": [
    "wandb.init(\n",
    "    project=\"LLM\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"First Full Model\",\n",
    "        \"dataset\": \"Shakespeare\",\n",
    "    },\n",
    "    name=\"First Full Model (Normal Encoding)\"\n",
    ")\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')"
   ],
   "id": "7a59642989e70cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 46,
   "source": "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)",
   "id": "842c35bdf7c9d219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 47,
   "source": [
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "tempreture = 1.0  # Tempreture in softmax\n",
    "# ------------"
   ],
   "id": "41da6292b8f6bc75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 48,
   "source": [
    "wandb.log({\"Context Length\": block_size, \"Batch Size\": batch_size, \"Learning Rate\": learning_rate, \"Dropout\": dropout,\n",
    "           \"Tempreture\": tempreture, \"Number of Heads\": n_head, \"Number of Layers\": n_layer, \"Embedding Size\": n_embd,\n",
    "           \"Eval Iterations\": eval_iters, \"Eval Intervals\": eval_interval, \"Number of Iterations\": max_iters})\n"
   ],
   "id": "6a349ad581ea76d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3139, val loss 4.3097\n",
      "step 100: train loss 2.6329, val loss 2.6476\n",
      "step 200: train loss 2.5032, val loss 2.5217\n",
      "step 300: train loss 2.4124, val loss 2.4178\n",
      "step 400: train loss 2.3461, val loss 2.3498\n",
      "step 500: train loss 2.3009, val loss 2.3141\n",
      "step 600: train loss 2.2415, val loss 2.2508\n",
      "step 700: train loss 2.2011, val loss 2.2086\n",
      "step 800: train loss 2.1572, val loss 2.1745\n",
      "step 900: train loss 2.1175, val loss 2.1533\n",
      "step 1000: train loss 2.0828, val loss 2.1311\n",
      "step 1100: train loss 2.0557, val loss 2.1001\n",
      "step 1200: train loss 2.0199, val loss 2.0709\n",
      "step 1300: train loss 2.0049, val loss 2.0613\n",
      "step 1400: train loss 1.9733, val loss 2.0570\n",
      "step 1500: train loss 1.9467, val loss 2.0338\n",
      "step 1600: train loss 1.9391, val loss 2.0320\n",
      "step 1700: train loss 1.9186, val loss 2.0116\n",
      "step 1800: train loss 1.9046, val loss 2.0019\n",
      "step 1900: train loss 1.8887, val loss 2.0037\n",
      "step 2000: train loss 1.8563, val loss 1.9713\n",
      "step 2100: train loss 1.8533, val loss 1.9679\n",
      "step 2200: train loss 1.8420, val loss 1.9636\n",
      "step 2300: train loss 1.8277, val loss 1.9476\n",
      "step 2400: train loss 1.8301, val loss 1.9358\n",
      "step 2500: train loss 1.8008, val loss 1.9281\n",
      "step 2600: train loss 1.7966, val loss 1.9197\n",
      "step 2700: train loss 1.7919, val loss 1.9200\n",
      "step 2800: train loss 1.7767, val loss 1.8981\n",
      "step 2900: train loss 1.7682, val loss 1.9009\n",
      "step 3000: train loss 1.7628, val loss 1.9001\n",
      "step 3100: train loss 1.7440, val loss 1.8930\n",
      "step 3200: train loss 1.7402, val loss 1.8770\n",
      "step 3300: train loss 1.7359, val loss 1.8735\n",
      "step 3400: train loss 1.7314, val loss 1.8708\n",
      "step 3500: train loss 1.7222, val loss 1.8762\n",
      "step 3600: train loss 1.7153, val loss 1.8728\n",
      "step 3700: train loss 1.7120, val loss 1.8712\n",
      "step 3800: train loss 1.7137, val loss 1.8701\n",
      "step 3900: train loss 1.7050, val loss 1.8488\n",
      "step 4000: train loss 1.7066, val loss 1.8435\n",
      "step 4100: train loss 1.7032, val loss 1.8381\n",
      "step 4200: train loss 1.6937, val loss 1.8440\n",
      "step 4300: train loss 1.6899, val loss 1.8464\n",
      "step 4400: train loss 1.6683, val loss 1.8300\n",
      "step 4500: train loss 1.6682, val loss 1.8286\n",
      "step 4600: train loss 1.6674, val loss 1.8021\n",
      "step 4700: train loss 1.6499, val loss 1.8131\n",
      "step 4800: train loss 1.6665, val loss 1.8091\n",
      "step 4900: train loss 1.6583, val loss 1.7978\n",
      "step 4999: train loss 1.6554, val loss 1.8182\n"
     ]
    }
   ],
   "execution_count": 49,
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        wandb.log({\"Iteration\": iter, \"Train Loss\": losses['train'], \"Val Loss\": losses['val']})\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "id": "158048c95940f983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "great me rack tower: if you give\n",
      "Befewed the goods me theaking.\n",
      "Second lialt thou. Carmessire! my streaders;\n",
      "\n",
      "KING RICIUS:\n",
      "The, when: so lords; my seat may king!\n",
      "\n",
      "KING EKE OF GlES\n",
      "\n",
      "DATHARD IIA:\n",
      "My came.\n",
      "\n",
      "ELY:\n",
      "\n",
      "LADY CAPIO:\n",
      "So palsess, your confal noce; to being wine a mant-of is bear,\n",
      "And bringbred, id me, at to gently?\n",
      "\n",
      "LEONTES:\n",
      "Near I im knowits: Retbress mead.\n",
      "\n",
      "SICINIUS:\n",
      "I live, and when, have you, names, do I know yield fight.'\n",
      "Proke: a let I faint--befitt, Reagse rongbrees,\n",
      "Tent ceensedings and no counnt, fatcouse and his theer\n",
      "And whesesh will to your hightly: Politiong I behapowsat we a heards:\n",
      "Harwfell! for sick you.\n",
      "The marry'st redesings and featheds,\n",
      "And you anboothing weopps? call could him the comforn how heirss;\n",
      "So do unger's to York; good my mover boes,\n",
      "Thei: going abjecaudience; you\n",
      "wre competibument soun prot dinesity!\n",
      "\n",
      "CORIOLA:\n",
      "Appilitss me!--low it made:\n",
      "A deathth thee, ifforges, my paaint,\n",
      "Where he can amernise. Dry own, who prin impation's chame,\n",
      "To I farther leaven the dongbragge?\n",
      "\n",
      "SARLING:\n",
      "Which wougalt my kishome, 'dy; where's dream too.\n",
      "\n",
      "DUKE Vencalling; and thy down: you not denepers.\n",
      "In marry having, here it, who, a callied;\n",
      "Yet 'tirrogiss a not poor and leady, all boes\n",
      "Tides Erequess, Godest marry loves,\n",
      "Not the deishs to bried by him, beest you face of that to\n",
      "more falther uponal, in a songercented word o' it a shan king--\n",
      "Then worsth'd usent livedness, I know they datisp-bum thought;\n",
      "Aren I well. O your not hisbardame! Gauray, for his bend so kishousager;\n",
      "Should were prenison, he kind thy.\n",
      "'Till Rome, set thy briets, and poor friend be,\n",
      "As your lover not is earough, frience.\n",
      "\n",
      "JULIET:\n",
      "Night-wardher?\n",
      "\n",
      "TRANIO:\n",
      "You bears.\n",
      "\n",
      "JOP\n",
      "MENENIURD't:\n",
      "I kin thou move thy reft in Camildy hither,\n",
      "Not to the; seave my tongue a-fortunes their worms\n",
      "gace hoon his patitage, if looks will incouul.\n",
      "\n",
      "BUMPE:\n",
      "What I ham, woubject, myst Lendio.\n",
      "\n",
      "queant plivy:\n",
      "I my you, I have kingsly brieght,\n",
      "Gart grieft, a way hadringstle at it bysengedings this day.\n",
      "\n",
      "PRINCE HENRY\n"
     ]
    }
   ],
   "execution_count": 57,
   "source": [
    "# generate from the model\n",
    "wandb.log({\"Device\": device})\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(normal_encoding.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "wandb.log({\"Generated Sequence\": normal_encoding.decode(m.generate(context, max_new_tokens=2000)[0].tolist())})\n",
    "\n",
    "wandb.finish()"
   ],
   "id": "b69358f3cf69282"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
